name: DQN RLAF Loop (LLM via GemmaTrainer, simplified outputs)
description: >
  Uses DQN to pick hyperparams, retrains Gemma LLM, evaluates from trainer report,
  and saves best weights to a single retrained_model output. Other artifacts are
  written next to that file automatically.

inputs:
  - {name: tokenizer_json, type: Model}
  - {name: train_corpus,   type: Data}
  - {name: model_config,   type: Data}
  - {name: model_weights,  type: Model}
  - {name: model_py_in,    type: Data}

  # DQN / instance wiring
  - {name: domain,            type: String}
  - {name: schema_id,         type: String}
  - {name: model_id,          type: String}
  - {name: dqn_pipeline_id,   type: String}
  - {name: dqn_experiment_id, type: String}
  - {name: pipeline_domain,   type: String}
  - {name: access_token,      type: string}

  # Optional seed metrics (from your Evaluate brick)
  - {name: init_metrics,      type: Metrics}

outputs:
  - {name: rlaf_output,     type: Dataset}
  - {name: retrained_model, type: Model}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v19
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os, json, time, argparse, csv, math, requests, shutil
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        from nesy_factory.language_model.train import GemmaTrainer

        # ---------------- HTTP helpers ----------------
        def get_retry_session():
            retry = Retry(total=5, status_forcelist=[500,502,503,504], backoff_factor=1)
            adapter = HTTPAdapter(max_retries=retry)
            s = requests.Session()
            s.mount("https://", adapter)
            s.mount("http://",  adapter)
            return s

        def get_instance(access_token, domain, schema_id, model_id):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v3.0/schemas/{schema_id}/instances/list"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {"dbType": "TIDB", "ownedOnly": True, "filter": {"model_id": model_id}}
            print(f"Get instance request URL: {url}, Headers: {headers}, Payload: {payload}")
            r = http.post(url, headers=headers, json=payload, timeout=30)
            r.raise_for_status()
            print(f"Get instance. Status Code: {r.status_code}, Full response: {r.json()}")
            return r.json()["content"][0]

        def update_instance_field(access_token, domain, schema_id, model_id, field, value):
            http = get_retry_session()
            url = f"{domain}/pi-entity-instances-service/v2.0/schemas/{schema_id}/instances"
            headers = {"Authorization": f"Bearer {access_token}", "Content-Type": "application/json"}
            payload = {
                "dbType":"TIDB",
                "conditionalFilter":{"conditions":[{"field":"model_id","operator":"EQUAL","value":model_id}]},
                "partialUpdateRequests":[{"patch":[{"operation":"REPLACE","path":f"{field}","value":value}]}]
            }
            print(f"Update instance field request - URL: {url}, Headers: {headers}, Payload: {json.dumps(payload)}")
            headers_str = " ".join([f"-H '{k}: {v}'" for k, v in headers.items()])
            print(f"curl -X PATCH '{url}' {headers_str} -d '{json.dumps(payload)}'")
            r = http.patch(url, headers=headers, data=json.dumps(payload), timeout=30)
            r.raise_for_status()
            print(f"Update instance field. Status Code: {r.status_code}, Full response: {r.json()}")
            print(f"Successfully updated field '{field}' for model_id '{model_id}'")

        def trigger_pipeline(config, pipeline_domain, dqn_params=None):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/trigger/ml?pipelineId={config['pipeline_id']}"
            parameters = {"param_json": json.dumps(dqn_params)} if dqn_params else {}
            payload = json.dumps({
                "pipelineType":"ML",
                "containerResources":{},
                "experimentId":config["experiment_id"],
                "enableCaching":True,
                "parameters":parameters,
                "version":1
            })
            headers={"accept":"application/json","Authorization":f"Bearer {config['access_token']}",
                     "Content-Type":"application/json"}
            print(f"Trigger pipeline request URL: {url}, Headers: {headers}, Payload: {payload}")
            r = http.post(url, headers=headers, data=payload, timeout=30)
            r.raise_for_status()
            print(f"Triggered pipeline. Status Code: {r.status_code}, Response: {r.json()}")
            return r.json()["runId"]

        def get_pipeline_status(config, pipeline_domain):
            http = get_retry_session()
            url = f"{pipeline_domain}/bob-service-test/v1.0/pipeline/{config['pipeline_id']}/status/ml/{config['run_id']}"
            headers={"accept":"application/json","Authorization":f"Bearer {config['access_token']}"}
            print(f"Get pipeline status request URL: {url}, Headers: {headers}")
            r = http.get(url, headers=headers, timeout=30)
            r.raise_for_status()
            pipeline_status = r.json()
            print(f"Get pipeline status. Status Code: {r.status_code}, Full response: {pipeline_status}")
            latest_state = pipeline_status["run_details"]["state_history"][-1]
            return latest_state["state"]

        def trigger_and_wait_for_dqn_pipeline(config, pipeline_domain, dqn_params):
            run_id = trigger_pipeline(config, pipeline_domain, dqn_params)
            config["run_id"] = run_id
            while True:
                s = get_pipeline_status(config, pipeline_domain)
                print(f"Current DQN pipeline status: {s}")
                if s == "SUCCEEDED":
                    print("DQN Pipeline execution completed.")
                    break
                if s in ["FAILED","ERROR"]:
                    raise RuntimeError(f"DQN Pipeline failed with status {s}")
                time.sleep(60)

        # ---------------- Metrics helpers ----------------
        def load_metrics_from_report(report, curve):
            metrics = {}
            if os.path.exists(report):
                try:
                    with open(report,"r") as f:
                        metrics = json.load(f)
                except Exception as e:
                    print("[warn] parse training_report:", e)
            if os.path.exists(curve):
                try:
                    with open(curve,"r") as f:
                        rows = list(csv.DictReader(f))
                        if rows:
                            last = rows[-1]
                            for k,v in last.items():
                                try:
                                    metrics.setdefault(k, float(v))
                                except:
                                    pass
                except Exception as e:
                    print("[warn] parse loss_curve.csv:", e)
            if "perplexity" not in metrics and "val_loss" in metrics:
                try:
                    metrics["perplexity"] = math.exp(float(metrics["val_loss"]))
                except:
                    pass
            return metrics

        def build_dqn_params(metrics_dict):
            dqn = []
            for key, value in metrics_dict.items():
                try:
                    float(value)
                    # Determine sign based on metric name
                    if "loss" in key.lower() or "perplexity" in key.lower():
                        sign = "-"
                    elif "accuracy" in key.lower() or "f1" in key.lower() or "bleu" in key.lower() or "rouge" in key.lower():
                        sign = "+"
                    else:
                        continue  # Skip metrics we don't recognize
                    dqn.append({"key": key, "sign": sign, "mul": 1.0})
                except (ValueError, TypeError):
                    print(f"Warning: Could not convert metric '{key}' with value '{value}' to float. Skipping.")
            
            # Return default if no valid metrics found
            return dqn if dqn else [{"key":"perplexity","sign":"-","mul":1.0}]

        # ---------------- Action handling ----------------
        def coerce_action_params(action_params: dict) -> dict:
            conv = {"learning_rate":float,"min_lr":float,"warmup_steps":int,"grad_accum":int,
                    "epochs":int,"error_absolute_threshold":float}
            out={}
            for k,v in (action_params or {}).items():
                try:
                    out[k] = conv.get(k, lambda x:x)(v)
                except:
                    out[k] = v
            if "epochs" in out and "max_iters" not in out:
                out["max_iters"] = out["epochs"]
            return out

        def resolve(param, action, cfg, defaults):
            if param in action: return action[param]
            if param in cfg:    return cfg[param]
            return defaults.get(param)

        def pick_threshold_violation(metrics, err_thresh):
            if err_thresh is None:
                return (False, None, None)
            try:
                thr = float(err_thresh)
            except:
                return (False, None, None)
            key = "perplexity" if "perplexity" in metrics else ("val_loss" if thr < 1.0 and "val_loss" in metrics else None)
            if key and key in metrics:
                try:
                    val = float(metrics[key])
                    return (val > thr, key, val)
                except:
                    return (False, key, None)
            return (False, None, None)

        # ---------------- Training wrapper ----------------
        def train_llm_with_action(action_params, paths, defaults, out_dir):
            with open(paths["model_config"], "r") as f:
                base_cfg = json.load(f)

            coerced = coerce_action_params(action_params)
            err_thresh = coerced.get("error_absolute_threshold")

            # Fixed filenames next to retrained_model
            model_py_out     = os.path.join(out_dir, "gemma3_model.out.py")
            best_weights     = os.path.join(out_dir, "best.pt")
            final_weights    = os.path.join(out_dir, "final.pt")
            training_report  = os.path.join(out_dir, "training_report.json")
            loss_curve_csv   = os.path.join(out_dir, "loss_curve.csv")

            os.makedirs(out_dir, exist_ok=True)

            kwargs = {
                "learning_rate": resolve("learning_rate", coerced, base_cfg, defaults),
                "min_lr":        resolve("min_lr",        coerced, base_cfg, defaults),
                "warmup_steps":  resolve("warmup_steps",  coerced, base_cfg, defaults),
                "grad_accum":    resolve("grad_accum",    coerced, base_cfg, defaults),
                "max_iters":     resolve("max_iters",     coerced, base_cfg, defaults),
                "batch_size":    resolve("batch_size",    coerced, base_cfg, defaults),
                "block_size":    resolve("block_size",    coerced, base_cfg, defaults),
                "eval_interval": resolve("eval_interval", coerced, base_cfg, defaults),
                "eval_iters":    resolve("eval_iters",    coerced, base_cfg, defaults),
                "weight_decay":  resolve("weight_decay",  coerced, base_cfg, defaults),
                "beta2":         resolve("beta2",         coerced, base_cfg, defaults),
                "clip_grad_norm":resolve("clip_grad_norm",coerced, base_cfg, defaults),
                "val_fraction":  resolve("val_fraction",  coerced, base_cfg, defaults),
                "num_proc":      resolve("num_proc",      coerced, base_cfg, defaults),

                "tokenizer_json": paths["tokenizer_json"],
                "train_corpus":   paths["train_corpus"],
                "model_config":   paths["model_config"],
                "model_weights":  paths["model_weights"],
                "model_py_in":    paths["model_py_in"],
                "model_py_out":   model_py_out,
                "best_weights":   best_weights,
                "final_weights":  final_weights,
                "training_report":training_report,
                "loss_curve_csv": loss_curve_csv,
            }

            print("[GemmaTrainer.run kwargs]", json.dumps({k:kwargs[k] for k in sorted(kwargs)}, indent=2))
            GemmaTrainer().run(**kwargs)

            metrics = load_metrics_from_report(training_report, loss_curve_csv)
            return metrics, err_thresh, best_weights

        # ---------------- Main ----------------
        def main():
            ap = argparse.ArgumentParser()
            # inputs
            ap.add_argument("--tokenizer_json", required=True)
            ap.add_argument("--train_corpus",   required=True)
            ap.add_argument("--model_config",   required=True)
            ap.add_argument("--model_weights",  required=True)
            ap.add_argument("--model_py_in",    required=True)

            ap.add_argument("--domain", required=True)
            ap.add_argument("--schema_id", required=True)
            ap.add_argument("--model_id", required=True)
            ap.add_argument("--dqn_pipeline_id", required=True)
            ap.add_argument("--dqn_experiment_id", required=True)
            ap.add_argument("--pipeline_domain", required=True)
            ap.add_argument("--access_token", required=True)

            # outputs (defaults enable local runs; orchestrator overrides via outputPath)
            ap.add_argument("--rlaf_output", default="./out/rlaf_output.json")
            ap.add_argument("--retrained_model", default="./out/retrained_model.pt")

            # optional seed metrics
            ap.add_argument("--init_metrics")
            args = ap.parse_args()

            os.makedirs(os.path.dirname(args.rlaf_output) or ".", exist_ok=True)
            os.makedirs(os.path.dirname(args.retrained_model) or ".", exist_ok=True)

            with open(args.access_token,"r") as f:
                access_token = f.read().strip()

            # Load initial metrics (handle both KFP format and plain dict)
            current_metrics = {}
            if args.init_metrics and os.path.exists(args.init_metrics):
                try:
                    with open(args.init_metrics,"r") as f:
                        data = json.load(f)
                    
                    # Check if it's KFP metrics format
                    if "metrics" in data and isinstance(data["metrics"], list):
                        # Convert KFP format to plain dict
                        for metric in data["metrics"]:
                            current_metrics[metric["name"]] = metric["numberValue"]
                    else:
                        # Assume it's already a plain dict
                        current_metrics = data
                    
                    print(f"[INFO] Loaded initial metrics: {current_metrics}")
                except Exception as e:
                    print(f"[warn] parse init_metrics: {e}")

            defaults = {
                "learning_rate": 1e-4, "min_lr": 1e-5, "warmup_steps": 1,
                "max_iters": 6, "batch_size": 8, "block_size": 64, "grad_accum": 1,
                "eval_interval": 1, "eval_iters": 5, "weight_decay": 0.1, "beta2": 0.95,
                "clip_grad_norm": 0.5, "val_fraction": 0.1, "num_proc": 6
            }

            paths = {
                "tokenizer_json": args.tokenizer_json,
                "train_corpus":   args.train_corpus,
                "model_config":   args.model_config,
                "model_weights":  args.model_weights,
                "model_py_in":    args.model_py_in,
            }

            out_dir = os.path.dirname(args.retrained_model) or "."

            action_id_for_next_pierce = -1
            
            for iteration in range(2):
                print(f"=" * 80)
                print(f" RLAF Loop Iteration {iteration + 1}")
                print(f"=" * 80)
                
                # Clean and prepare metrics for DQN
                cleaned_metrics = {}
                fixed_keys = [("bleu_score", "+"), ("rouge_score", "+"), ("perplexity", "-")]
                for k, _ in fixed_keys:
                  cleaned_metrics.setdefault(k, 0.0)  # If 'perplexity' is missing, it will be set to 0.0

                dqn_params = [{"key": k, "sign": s, "mul": 1.0} for k, s in fixed_keys]
                # cleaned_metrics = {}
                # for key, value in current_metrics.items():
                #     try:
                #         cleaned_metrics[key] = float(value)
                #     except (ValueError, TypeError):
                #         print(f"Warning: Could not convert metric '{key}' with value '{value}' to float. Skipping.")
                
                # dqn_params = build_dqn_params(cleaned_metrics)
                print(f"Dynamically generated param_json for DQN: {json.dumps(dqn_params)}")

                # Get current instance state
                inst = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                print(f"Current instance: {inst}")
                
                if inst.get("pierce2rlaf"):
                    last = inst["pierce2rlaf"][-1]
                    previous_state = last.get("current_state", cleaned_metrics)
                    episode = last.get("episode", 0)
                else:
                    previous_state = {key: 0.0 for key in cleaned_metrics.keys()}
                    episode = 0

                # Update pierce2rlaf with current state
                new_entry = {
                    "action_id": action_id_for_next_pierce,
                    "previous_state": previous_state,
                    "current_state": cleaned_metrics,
                    "episode": episode,
                    "timestamp": int(time.time())
                }
                hist = inst.get("pierce2rlaf", [])
                hist.append(new_entry)
                update_instance_field(access_token, args.domain, args.schema_id, args.model_id, "pierce2rlaf", hist)

                # Trigger DQN pipeline
                dqn_cfg = {
                    "pipeline_id": args.dqn_pipeline_id,
                    "experiment_id": args.dqn_experiment_id,
                    "access_token": access_token
                }
                print(f"DQN Config: {dqn_cfg}")
                trigger_and_wait_for_dqn_pipeline(dqn_cfg, args.pipeline_domain, dqn_params)

                # Get DQN recommendation
                updated = get_instance(access_token, args.domain, args.schema_id, args.model_id)
                print(f"{updated}")
                latest = updated["rlaf2pierce"][-1]
                
                if not latest.get("pierce_or_not", True):
                    print("pierce_or_not is false. Exiting loop.")
                    break

                print(f"Latest rlaf2pierce: {latest}")
                
                action_id_for_next_pierce = latest["action_id"]
                actions = updated.get("rlaf_actions", {}).get("actions", [])
                action = next((a for a in actions if a.get("id") == action_id_for_next_pierce), None)
                if not action:
                    raise ValueError(f"Action with ID {action_id_for_next_pierce} not found in rlaf_actions")

                print(f"DQN pipeline recommended action: {action}. Retraining model.")
                
                # Train with recommended action
                metrics, err_thresh, best_path = train_llm_with_action(action.get("params", {}), paths, defaults, out_dir)

                # Calculate improvement score
                improvement_score = 0.0
                for p in dqn_params:
                    k = p["key"]
                    sign = 1 if p["sign"] == "+" else -1
                    if k in metrics and k in previous_state:
                        try:
                            improvement = (float(metrics[k]) - float(previous_state[k])) * sign
                            improvement_score += improvement
                        except:
                            pass

                # Check threshold violation
                viol, key_used, val_used = pick_threshold_violation(metrics, err_thresh)
                if key_used is not None:
                    print(f"[Gate] {key_used}={val_used} vs threshold={err_thresh} â†’ {'VIOLATE' if viol else 'OK'}")

                # Decide whether to keep model
                if improvement_score > 0 and not viol:
                    print(f"Metrics improved (score: {improvement_score:.4f}). Saving model.")
                    if os.path.abspath(best_path) != os.path.abspath(args.retrained_model):
                        shutil.copy2(best_path, args.retrained_model)
                    print(f"Saved retrained model to {args.retrained_model}")
                else:
                    reason = "no improvement" if improvement_score <= 0 else "threshold violated"
                    print(f"No improvement in metrics (reason: {reason}, score: {improvement_score:.4f}). Model not saved.")
                    os.makedirs(os.path.dirname(args.retrained_model), exist_ok=True)
                    with open(args.retrained_model, "w") as f:
                        f.write("Model not saved due to lack of improvement.")

                current_metrics = metrics

            # Write final output
            with open(args.rlaf_output,"w") as f:
                json.dump({"final_metrics": current_metrics}, f, indent=4)
            print(f"RLAF loop finished. Final parameters written to {args.rlaf_output}")

        if __name__ == "__main__":
            main()
    args:
      # inputs (read-only)
      - --tokenizer_json
      - {inputPath: tokenizer_json}
      - --train_corpus
      - {inputPath: train_corpus}
      - --model_config
      - {inputPath: model_config}
      - --model_weights
      - {inputPath: model_weights}
      - --model_py_in
      - {inputPath: model_py_in}

      # dqn wiring
      - --domain
      - {inputValue: domain}
      - --schema_id
      - {inputValue: schema_id}
      - --model_id
      - {inputValue: model_id}
      - --dqn_pipeline_id
      - {inputValue: dqn_pipeline_id}
      - --dqn_experiment_id
      - {inputValue: dqn_experiment_id}
      - --pipeline_domain
      - {inputValue: pipeline_domain}
      - --access_token
      - {inputPath: access_token}

      # simplified outputs (writable; orchestrator injects paths)
      - --rlaf_output
      - {outputPath: rlaf_output}
      - --retrained_model
      - {outputPath: retrained_model}

      # optional seed metrics
      - --init_metrics
      - {inputPath: init_metrics}
