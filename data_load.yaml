name: HF Text Dataset â†’ TXT Exporter
description: Loads a Hugging Face text dataset split and writes it to a newline-delimited .txt file (one example per line).
inputs:
  - {name: Dataset Name, type: String, default: 'roneneldan/TinyStories', description: 'Hugging Face dataset path, e.g., roneneldan/TinyStories or ag_news', optional: false}
  - {name: Config, type: String, default: '', description: 'Optional dataset config (leave blank if none)', optional: true}
  - {name: Split, type: String, default: 'train', description: 'Split to export (e.g., train/validation/test)', optional: true}
  - {name: Text Fields, type: String, default: '', description: 'Comma-separated text columns to export. If blank, will try to auto-detect', optional: true}
  - {name: Separator, type: String, default: ' ', description: 'Separator to join multiple text fields', optional: true}
  - {name: Streaming, type: Boolean, default: 'false', description: 'Use streaming mode for very large datasets', optional: true}
  - {name: Max Rows, type: Integer, default: '0', description: 'Optional cap on rows to write (0 means no cap)', optional: true}
outputs:
  - {name: Text File, type: Data, description: 'Plaintext .txt file containing one example per line from the chosen split'}
implementation:
  container:
    image: python:3.9
    command:
      - bash
      - -ec
      - |
        set -o pipefail
        export PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet datasets tqdm || python3 -m pip install --quiet --user datasets tqdm

        # Write the Python exporter to disk
        cat >/tmp/program.py <<'PY'
        import argparse, os, sys
        from typing import List, Optional
        from datasets import load_dataset, Features, Value, IterableDataset, Dataset

        try:
            from tqdm import tqdm
        except Exception:
            def tqdm(x, **k): return x  # no-op fallback

        def guess_text_fields(features: Optional[Features], prefer: List[str]) -> List[str]:
            if features is None:
                return prefer
            candidates = [name for name, feat in features.items()
                          if isinstance(feat, Value) and feat.dtype in ("string", "large_string")]
            if candidates:
                return candidates
            for n in prefer:
                if n in features:
                    return [n]
            for name, feat in features.items():
                if isinstance(feat, Value):
                    return [name]
            return []

        def build_text(example: dict, fields: List[str], sep: str) -> Optional[str]:
            parts = []
            for f in fields:
                v = example.get(f, None)
                if v is None:
                    continue
                if isinstance(v, list):
                    v = " ".join(map(str, v))
                parts.append(str(v))
            if not parts:
                return None
            s = sep.join(parts).replace("\r\n", "\n").replace("\r", "\n").strip("\n")
            return s

        def export_iter(ds_iter, out_path: str, fields: List[str], sep: str, max_rows: Optional[int]):
            n = 0
            with open(out_path, "w", encoding="utf-8") as f:
                for ex in tqdm(ds_iter, desc="Writing", unit="rows"):
                    s = build_text(ex, fields, sep)
                    if s:
                        f.write(s + "\n")
                        n += 1
                        if max_rows and n >= max_rows:
                            break
            return n

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--dataset-name", required=True)
            ap.add_argument("--config", default="")
            ap.add_argument("--split", default="train")
            ap.add_argument("--text-fields", default="")
            ap.add_argument("--sep", default=" ")
            ap.add_argument("--streaming", action="store_true")
            ap.add_argument("--max-rows", type=int, default=0)
            ap.add_argument("--output-path", required=True)
            args = ap.parse_args()

            os.makedirs(os.path.dirname(args.output_path) or ".", exist_ok=True)

            prefer = ["text", "content", "sentence", "document", "article", "title"]

            cfg = args.config or None
            ds = load_dataset(args.dataset_name, cfg, split=args.split, streaming=args.streaming)

            if args.text_fields.strip():
                fields = [x.strip() for x in args.text_fields.split(",") if x.strip()]
            else:
                features = getattr(ds, "features", None)
                fields = guess_text_fields(features, prefer)
                if not fields:
                    print("ERROR: Could not infer text fields. Provide --text-fields.", file=sys.stderr)
                    sys.exit(2)

            max_rows = args.max_rows if args.max_rows > 0 else None

            if isinstance(ds, IterableDataset):
                n = export_iter(ds, args.output_path, fields, args.sep, max_rows)
            else:
                n = export_iter(ds, args.output_path, fields, args.sep, max_rows)

            print(f"[SUCCESS] Wrote {n} lines to {args.output_path}")
            print(f"[INFO] Using fields: {fields}")

        if __name__ == "__main__":
            main()
        PY

        # Run the exporter.
        # NOTE: For "bash -c", $0 is the first arg after the script, $1 is the next, etc.
        python3 -u /tmp/program.py \
          --dataset-name "$0" \
          --config "$1" \
          --split "$2" \
          --text-fields "$3" \
          --sep "$4" \
          $(test "$5" = "true" && echo --streaming || true) \
          $(test "$6" -gt 0 2>/dev/null && echo --max-rows "$6" || true) \
          --output-path "$7"
    args:
      - {inputValue: Dataset Name}
      - {inputValue: Config}
      - {inputValue: Split}
      - {inputValue: Text Fields}
      - {inputValue: Separator}
      - {inputValue: Streaming}
      - {inputValue: Max Rows}
      - {outputPath: Text File}
