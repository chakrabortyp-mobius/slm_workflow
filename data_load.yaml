name: HuggingFace Dataset Text Exporter
description: Exports any Hugging Face dataset to a newline-delimited text file with automatic text field detection and configurable export options.

inputs:
  - {name: Dataset Name, type: String, default: 'roneneldan/TinyStories', description: 'Dataset name from Hugging Face Hub (e.g., roneneldan/TinyStories, ag_news)'}
  - {name: Config Name, type: String, default: '', description: 'Dataset configuration name (leave empty if not required)'}
  - {name: Split Name, type: String, default: 'train', description: 'Dataset split to export (train, validation, test, etc.)'}
  - {name: Text Fields, type: String, default: '', description: 'Comma-separated text field names to export (auto-detected if empty)'}
  - {name: Field Separator, type: String, default: ' ', description: 'Separator for joining multiple text fields'}
  - {name: Max Rows, type: Integer, default: '0', description: 'Maximum rows to export (0 for unlimited)'}
  - {name: Use Streaming, type: Boolean, default: 'false', description: 'Enable streaming mode for very large datasets'}

outputs:
  - {name: Text File, type: Data, description: 'Exported dataset as newline-delimited text file'}
  - {name: Export Metadata, type: Data, description: 'Export statistics and metadata in JSON format'}

implementation:
  container:
    image: python:3.9-slim
    command:
      - bash
      - -ec
      - |
        set -e
        
        # Install required packages
        echo "[INFO] Installing required packages..."
        pip install --quiet --no-cache-dir datasets tqdm
        
        # Create the export script
        cat > /tmp/hf_dataset_exporter.py << 'EOF'
        #!/usr/bin/env python3
        """
        Hugging Face Dataset to Text File Exporter
        Exports any HuggingFace dataset to a newline-delimited text file.
        """

        import argparse
        import json
        import os
        import sys
        from datetime import datetime
        from typing import List, Optional, Union

        try:
            from datasets import load_dataset, Features, Value, IterableDataset, Dataset
        except ImportError:
            print("Error: 'datasets' library not found. Install with: pip install datasets")
            sys.exit(1)

        try:
            from tqdm import tqdm
        except ImportError:
            # Fallback if tqdm is not available
            tqdm = lambda x, **kwargs: x


        class DatasetTextExporter:
            """Handles exporting Hugging Face datasets to text files."""
            
            def __init__(self):
                self.exported_count = 0
                self.start_time = None
                self.end_time = None
            
            def detect_text_fields(self, dataset, preferred_fields: List[str] = None) -> List[str]:
                """Automatically detect text fields in the dataset."""
                if preferred_fields is None:
                    preferred_fields = ["text", "content", "sentence", "document", "article", "body", "message"]
                
                # Try to get features from the dataset
                features = getattr(dataset, "features", None)
                
                if features is None:
                    # For streaming datasets or when features aren't available
                    print("[INFO] Features not available, using preferred field names")
                    return preferred_fields
                
                # Find string/text columns
                text_fields = []
                for name, feature in features.items():
                    if isinstance(feature, Value) and feature.dtype in ("string", "large_string"):
                        text_fields.append(name)
                
                # If no explicit string fields found, try preferred names
                if not text_fields:
                    for field_name in preferred_fields:
                        if field_name in features:
                            text_fields.append(field_name)
                
                # Last resort: take first available field
                if not text_fields and features:
                    first_field = list(features.keys())[0]
                    text_fields = [first_field]
                    print(f"[WARNING] Using first available field: {first_field}")
                
                return text_fields
            
            def process_example(self, example: dict, text_fields: List[str], separator: str = " ") -> Optional[str]:
                """Process a single example and extract text content."""
                text_parts = []
                
                for field in text_fields:
                    value = example.get(field)
                    if value is None:
                        continue
                        
                    # Handle different data types
                    if isinstance(value, list):
                        # Join list elements
                        text = " ".join(str(item) for item in value if item is not None)
                    else:
                        text = str(value)
                    
                    if text.strip():
                        text_parts.append(text.strip())
                
                if not text_parts:
                    return None
                    
                # Join all parts and clean up
                result = separator.join(text_parts)
                result = result.replace('\r\n', '\n').replace('\r', '\n')
                return result.strip()
            
            def export_dataset(self, 
                              dataset_name: str,
                              output_file: str,
                              config_name: str = None,
                              split_name: str = "train",
                              text_fields: List[str] = None,
                              separator: str = " ",
                              max_rows: int = None,
                              streaming: bool = False) -> dict:
                """Export dataset to text file and return metadata."""
                
                self.start_time = datetime.now()
                print(f"[INFO] Starting export of {dataset_name}")
                
                try:
                    # Load dataset
                    print(f"[INFO] Loading dataset: {dataset_name}")
                    if config_name:
                        print(f"[INFO] Using config: {config_name}")
                    print(f"[INFO] Using split: {split_name}")
                    
                    dataset = load_dataset(
                        dataset_name, 
                        config_name, 
                        split=split_name, 
                        streaming=streaming
                    )
                    
                    # Detect text fields if not provided
                    if not text_fields:
                        text_fields = self.detect_text_fields(dataset)
                        if not text_fields:
                            raise ValueError("Could not detect any text fields in the dataset")
                    
                    print(f"[INFO] Using text fields: {text_fields}")
                    print(f"[INFO] Using separator: '{separator}'")
                    if max_rows:
                        print(f"[INFO] Maximum rows to export: {max_rows}")
                    
                    # Create output directory if needed
                    os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)
                    
                    # Export data
                    self.exported_count = 0
                    with open(output_file, 'w', encoding='utf-8') as f:
                        iterator = tqdm(dataset, desc="Exporting", unit="rows") if not streaming else dataset
                        
                        for example in iterator:
                            processed_text = self.process_example(example, text_fields, separator)
                            
                            if processed_text:
                                f.write(processed_text + '\n')
                                self.exported_count += 1
                                
                                if max_rows and self.exported_count >= max_rows:
                                    print(f"[INFO] Reached maximum row limit: {max_rows}")
                                    break
                    
                    self.end_time = datetime.now()
                    duration = (self.end_time - self.start_time).total_seconds()
                    
                    # Prepare metadata
                    metadata = {
                        "dataset_name": dataset_name,
                        "config_name": config_name,
                        "split_name": split_name,
                        "text_fields": text_fields,
                        "separator": separator,
                        "streaming_mode": streaming,
                        "max_rows": max_rows,
                        "exported_rows": self.exported_count,
                        "output_file": output_file,
                        "start_time": self.start_time.isoformat(),
                        "end_time": self.end_time.isoformat(),
                        "duration_seconds": duration,
                        "status": "success"
                    }
                    
                    print(f"[SUCCESS] Exported {self.exported_count} rows to {output_file}")
                    print(f"[SUCCESS] Export completed in {duration:.2f} seconds")
                    
                    return metadata
                    
                except Exception as e:
                    self.end_time = datetime.now()
                    error_metadata = {
                        "dataset_name": dataset_name,
                        "config_name": config_name,
                        "split_name": split_name,
                        "error": str(e),
                        "start_time": self.start_time.isoformat() if self.start_time else None,
                        "end_time": self.end_time.isoformat(),
                        "status": "failed"
                    }
                    
                    print(f"[ERROR] Export failed: {e}")
                    return error_metadata


        def main():
            parser = argparse.ArgumentParser(description="Export Hugging Face dataset to text file")
            parser.add_argument("--dataset", required=True, help="Dataset name")
            parser.add_argument("--output", required=True, help="Output text file path")
            parser.add_argument("--config", default=None, help="Dataset configuration name")
            parser.add_argument("--split", default="train", help="Dataset split")
            parser.add_argument("--text-fields", help="Comma-separated text field names")
            parser.add_argument("--separator", default=" ", help="Field separator")
            parser.add_argument("--max-rows", type=int, help="Maximum rows to export")
            parser.add_argument("--streaming", action="store_true", help="Use streaming mode")
            parser.add_argument("--metadata-output", help="Metadata output file")
            
            args = parser.parse_args()
            
            # Parse text fields
            text_fields = None
            if args.text_fields:
                text_fields = [field.strip() for field in args.text_fields.split(',') if field.strip()]
            
            # Create exporter and run
            exporter = DatasetTextExporter()
            metadata = exporter.export_dataset(
                dataset_name=args.dataset,
                output_file=args.output,
                config_name=args.config if args.config else None,
                split_name=args.split,
                text_fields=text_fields,
                separator=args.separator,
                max_rows=args.max_rows,
                streaming=args.streaming
            )
            
            # Save metadata if requested
            if args.metadata_output:
                os.makedirs(os.path.dirname(args.metadata_output) or '.', exist_ok=True)
                with open(args.metadata_output, 'w', encoding='utf-8') as f:
                    json.dump(metadata, f, indent=2)
                print(f"[INFO] Metadata saved to {args.metadata_output}")
            
            # Exit with error code if export failed
            if metadata.get("status") == "failed":
                sys.exit(1)

        if __name__ == "__main__":
            main()
        EOF
        
        # Make the script executable
        chmod +x /tmp/hf_dataset_exporter.py
        
        # Parse arguments and set defaults
        DATASET_NAME="$0"
        CONFIG_NAME="$1"
        SPLIT_NAME="$2"
        TEXT_FIELDS="$3"
        SEPARATOR="$4"
        MAX_ROWS="$5"
        USE_STREAMING="$6"
        OUTPUT_FILE="$7"
        METADATA_FILE="$8"
        
        # Build command arguments
        CMD_ARGS="--dataset \"$DATASET_NAME\" --output \"$OUTPUT_FILE\" --metadata-output \"$METADATA_FILE\""
        
        if [ -n "$CONFIG_NAME" ] && [ "$CONFIG_NAME" != "" ]; then
            CMD_ARGS="$CMD_ARGS --config \"$CONFIG_NAME\""
        fi
        
        if [ -n "$SPLIT_NAME" ] && [ "$SPLIT_NAME" != "" ]; then
            CMD_ARGS="$CMD_ARGS --split \"$SPLIT_NAME\""
        fi
        
        if [ -n "$TEXT_FIELDS" ] && [ "$TEXT_FIELDS" != "" ]; then
            CMD_ARGS="$CMD_ARGS --text-fields \"$TEXT_FIELDS\""
        fi
        
        if [ -n "$SEPARATOR" ] && [ "$SEPARATOR" != "" ]; then
            CMD_ARGS="$CMD_ARGS --separator \"$SEPARATOR\""
        fi
        
        if [ -n "$MAX_ROWS" ] && [ "$MAX_ROWS" != "0" ]; then
            CMD_ARGS="$CMD_ARGS --max-rows $MAX_ROWS"
        fi
        
        if [ "$USE_STREAMING" = "true" ]; then
            CMD_ARGS="$CMD_ARGS --streaming"
        fi
        
        # Execute the export
        echo "[INFO] Executing: python3 /tmp/hf_dataset_exporter.py $CMD_ARGS"
        eval "python3 /tmp/hf_dataset_exporter.py $CMD_ARGS"

  args:
    - {inputValue: Dataset Name}
    - {inputValue: Config Name}
    - {inputValue: Split Name}
    - {inputValue: Text Fields}
    - {inputValue: Field Separator}
    - {inputValue: Max Rows}
    - {inputValue: Use Streaming}
    - {outputPath: Text File}
    - {outputPath: Export Metadata}
