name: GRPOfinetune
description: Trains a LoRA fine-tuned model using GRPO with Unsloth for tool-calling improvement

inputs:
  - {name: base_model_name, type: String}
  - {name: lora_adapters, type: Model}
  - {name: dataset, type: Dataset}
  - {name: max_seq_length, type: Integer}
  - {name: lora_rank, type: Integer}
  - {name: learning_rate, type: Float}
  - {name: num_train_steps, type: Integer}
  - {name: batch_size, type: Integer}
  - {name: grad_accum_steps, type: Integer}
  - {name: num_generations, type: Integer}
  - {name: temperature, type: Float}
  - {name: tool_call_start, type: String}
  - {name: tool_call_end, type: String}
  - {name: think_start, type: String}
  - {name: think_end, type: String}
  - {name: custom_chat_template, type: String}

outputs:
  - {name: grpo_model, type: Model}



implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v24-gpu
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import torch
        import re
        import numpy as np
        from datasets import load_dataset, Dataset
        from unsloth import FastLanguageModel
        from vllm import SamplingParams
        from trl import GRPOConfig, GRPOTrainer
        import argparse
        import json
        import pickle

        def check_gpu():
            if torch.cuda.is_available():
                print("=============================")
                print("GPU Status:")
                for i in range(torch.cuda.device_count()):
                    print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
                    total_mem = torch.cuda.get_device_properties(i).total_memory
                    print(f"  VRAM: {total_mem / 1e9:.2f} GB")
                print("=======================================")
            else:
                print("WARNING: GPU is not available!")

        def main():
            check_gpu()
            
            parser = argparse.ArgumentParser()
            parser.add_argument('--base_model_name', type=str, required=True)
            parser.add_argument('--lora_adapters', type=str, required=True)
            parser.add_argument('--dataset', type=str, required=True)
            parser.add_argument('--max_seq_length', type=int, required=True)
            parser.add_argument('--lora_rank', type=int, required=True)
            parser.add_argument('--learning_rate', type=float, required=True)
            parser.add_argument('--num_train_steps', type=int, required=True)
            parser.add_argument('--batch_size', type=int, required=True)
            parser.add_argument('--grad_accum_steps', type=int, required=True)
            parser.add_argument('--num_generations', type=int, required=True)
            parser.add_argument('--temperature', type=float, required=True)
            parser.add_argument('--tool_call_start', type=str, required=True)
            parser.add_argument('--tool_call_end', type=str, required=True)
            parser.add_argument('--think_start', type=str, required=True)
            parser.add_argument('--think_end', type=str, required=True)
            parser.add_argument('--grpo_model', type=str, required=True)
            parser.add_argument('--custom_chat_template', type=str, required=True)
            args = parser.parse_args()

            print("==========================================================")
            print("GRPO Training Configuration:")
            print(f"  Base Model: {args.base_model_name}")
            print(f"  LoRA Adapters: {args.lora_adapters}")
            print(f"  Max Seq Length: {args.max_seq_length}")
            print(f"  Learning Rate: {args.learning_rate}")
            print(f"  Training Steps: {args.num_train_steps}")
            print(f"  Generations per prompt: {args.num_generations}")
            print("==============================================")

            # Load dataset
            print(f"Loading dataset from {args.dataset}")
            with open(args.dataset, 'rb') as f:
                dataset = pickle.load(f)
            print(f"Dataset loaded: {len(dataset)} examples")

            # Load model with Unsloth (2x faster!)
            print("Loading model with Unsloth...")
            model, tokenizer = FastLanguageModel.from_pretrained(
                model_name=args.base_model_name,
                max_seq_length=args.max_seq_length,
                load_in_4bit=False,  # Set to True if low on VRAM
                fast_inference=True,
                max_lora_rank=args.lora_rank,
                gpu_memory_utilization=0.9,
            )
            print("Base model loaded successfully")

            # Load LoRA adapters from previous SFT training
            adapter_path = os.path.join(args.lora_adapters, "adapter")
            if os.path.exists(adapter_path):
                print(f"Loading LoRA adapters from {adapter_path}")
                from peft import PeftModel
                model = PeftModel.from_pretrained(model, adapter_path)
                print("LoRA adapters loaded successfully")
            else:
                print(f"WARNING: No adapter found at {adapter_path}")

            # Use the SAME chat template from your LoRA fine-tuning
            # This ensures consistency between SFT and GRPO training
            if args.custom_chat_template and args.custom_chat_template.strip() and args.custom_chat_template.strip() != "None":
                template = args.custom_chat_template
                tokenizer.chat_template = template
                print("Custom chat template configured from input (matching SFT training)")
            else:
                print("No custom chat template provided. Using tokenizer's existing chat_template (from base model / adapters).")
            

            # Define reward functions for tool calling quality


            # Reward function 1: Exact format matching (highest reward)
            def match_format_exactly(completions, **kwargs):
                scores = []
                for completion in completions:
                    score = 0
                    response = completion[0]["content"]
                    # Check if response has proper structure
                    if match_format.search(response) is not None:
                        score += 3.0  # High reward for proper format
                    scores.append(score)
                return scores

            # Reward function 2: Approximate format matching
            def match_format_approximately(completions, **kwargs):
                scores = []
                for completion in completions:
                    score = 0
                    response = completion[0]["content"]
                    # Check for presence of each tag
                    score += 0.5 if response.count(args.think_end) == 1 else -1.0
                    score += 0.5 if response.count(args.tool_call_start) == 1 else -1.0
                    score += 0.5 if response.count(args.tool_call_end) == 1 else -1.0
                    scores.append(score)
                return scores

            # Reward function 3: Valid JSON in tool call
            def check_tool_call_validity(completions, **kwargs):
                scores = []
                for completion in completions:
                    score = 0
                    response = completion[0]["content"]
                    # Extract tool call content
                    match = match_format.search(response)
                    if match:
                        tool_call_content = match.group(1)
                        try:
                            # Try to parse as JSON
                            json.loads(tool_call_content)
                            score += 2.0  # Reward for valid JSON
                        except:
                            score -= 1.0  # Penalty for invalid JSON
                    else:
                        score -= 1.5
                    scores.append(score)
                return scores

            # Reward function 4: Answer correctness (if ground truth available)
            global PRINTED_TIMES, PRINT_EVERY_STEPS
            PRINTED_TIMES = 0
            PRINT_EVERY_STEPS = 5

            def check_answer_correctness(prompts, completions, answer, **kwargs):
                global PRINTED_TIMES
                if PRINTED_TIMES % PRINT_EVERY_STEPS == 0:
                    question = prompts[0][-1]["content"]
                    response = completions[0][0]["content"]
                    print(f"\\n{'===================================================='}")
                    print(f"Question:\\n{question}")
                    print(f"\\nAnswer:\\n{answer[0]}")
                    print(f"\\nResponse:\\n{response[:500]}...")
                    print('=======================================================================')
                PRINTED_TIMES += 1
                
                scores = []
                for completion, true_answer in zip(completions, answer):
                    response = completion[0]["content"]
                    # Simple string matching - customize based on your needs
                    if true_answer.lower() in response.lower():
                        scores.append(5.0)  # High reward for correct answer
                    else:
                        scores.append(-2.0)  # Penalty for wrong answer
                return scores

            # The dataset has "prompt" and "answer" as formatted strings with chat template already applied
            # We need to tokenize them directly
            print("Processing dataset for GRPO...")
            
            # Filter by length based on tokenized prompt
            print("Filtering dataset by token length...")
            def get_token_length(example):
                # Since prompt is already a formatted string, tokenize it directly
                tokens = tokenizer(example["prompt"], add_special_tokens=False)["input_ids"]
                return {"L": len(tokens)}
            
            tokenized = dataset.map(get_token_length)
            maximum_length = int(np.quantile(tokenized["L"], 0.9))
            dataset = dataset.select(
                np.where(np.array(tokenized["L"]) <= maximum_length)[0]
            )
            print(f"Filtered dataset: {len(dataset)} examples (max length: {maximum_length})")

            # GRPO configuration
            max_prompt_length = maximum_length + 1
            max_completion_length = args.max_seq_length - max_prompt_length

            vllm_sampling_params = SamplingParams(
                min_p=0.1,
                top_p=1.0,
                top_k=-1,
                seed=3407,
                stop=[tokenizer.eos_token],
                include_stop_str_in_output=True,
            )

            training_args = GRPOConfig(
                vllm_sampling_params=vllm_sampling_params,
                temperature=args.temperature,
                learning_rate=args.learning_rate,
                weight_decay=0.001,
                warmup_ratio=0.1,
                lr_scheduler_type="linear",
                optim="adamw_8bit",  # Memory-efficient optimizer
                logging_steps=1,
                per_device_train_batch_size=args.batch_size,
                gradient_accumulation_steps=args.grad_accum_steps,
                num_generations=args.num_generations,
                max_prompt_length=max_prompt_length,
                max_completion_length=max_completion_length,
                max_steps=args.num_train_steps,
                save_steps=args.num_train_steps,  # Save at end
                report_to="none",
                output_dir="outputs",
            )

            print("===========================================================")
            print("Initializing GRPO Trainer...")
            print(f"  Prompt length: {max_prompt_length}")
            print(f"  Completion length: {max_completion_length}")
            print(f"  Effective batch size: {args.batch_size * args.grad_accum_steps}")
            print("============================================================")

            trainer = GRPOTrainer(
                model=model,
                processing_class=tokenizer,
                reward_funcs=[
                    match_format_exactly,
                    match_format_approximately,
                    check_tool_call_validity,
                    check_answer_correctness,
                ],
                args=training_args,
                train_dataset=dataset,
            )

            print("\\nStarting GRPO training...")
            trainer.train()

            # Save trained model
            os.makedirs(os.path.dirname(args.grpo_model), exist_ok=True)
            save_path = os.path.join(args.grpo_model, "adapter")
            os.makedirs(save_path, exist_ok=True)
            
            print(f"\\nSaving GRPO-trained model to {save_path}")
            model.save_pretrained(save_path)
            tokenizer.save_pretrained(args.grpo_model)
            
            # Save training info
            training_info = {
                "base_model": args.base_model_name,
                "training_steps": args.num_train_steps,
                "learning_rate": args.learning_rate,
                "num_generations": args.num_generations,
                "final_dataset_size": len(dataset),
                "max_prompt_length": max_prompt_length,
                "max_completion_length": max_completion_length,
            }
            with open(os.path.join(args.grpo_model, "grpo_info.json"), "w") as f:
                json.dump(training_info, f, indent=2)

            print("=============================================================")
            print("GRPO training complete!")
            print(f"Model saved to: {args.grpo_model}")
            print("==============================================================")

        if __name__ == '__main__':
            main()
    args:
      - --base_model_name
      - {inputValue: base_model_name}
      - --lora_adapters
      - {inputPath: lora_adapters}
      - --dataset
      - {inputPath: dataset}
      - --max_seq_length
      - {inputValue: max_seq_length}
      - --lora_rank
      - {inputValue: lora_rank}
      - --learning_rate
      - {inputValue: learning_rate}
      - --num_train_steps
      - {inputValue: num_train_steps}
      - --batch_size
      - {inputValue: batch_size}
      - --grad_accum_steps
      - {inputValue: grad_accum_steps}
      - --num_generations
      - {inputValue: num_generations}
      - --temperature
      - {inputValue: temperature}
      - --tool_call_start
      - {inputValue: tool_call_start}
      - --tool_call_end
      - {inputValue: tool_call_end}
      - --think_start
      - {inputValue: think_start}
      - --think_end
      - {inputValue: think_end}
      - --custom_chat_template
      - {inputValue: custom_chat_template}
      - --grpo_model
      - {outputPath: grpo_model}
