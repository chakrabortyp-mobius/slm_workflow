name: GRPO Training with Unsloth
description: Continues training from SFT LoRA using GRPO with Unsloth

inputs:
  - {name: base_model_path, type: Model, description: 'Base model with resized embeddings from SFT'}
  - {name: lora_adapters_path, type: Model, description: 'SFT LoRA adapters to continue from'}
  - {name: tokenizer_path, type: Model, description: 'Tokenizer with special tokens from SFT'}
  - {name: grpo_dataset, type: Dataset, description: 'Pickle dataset with prompt and answer'}
  - {name: max_seq_length, type: Integer, description: 'Maximum sequence length (e.g., 2048)'}
  - {name: lora_rank, type: Integer, description: 'LoRA rank from SFT (e.g., 16 or 32)'}
  - {name: learning_rate, type: Float, description: 'Learning rate (e.g., 5e-6)'}
  - {name: num_train_steps, type: Integer, description: 'Training steps (e.g., 100-500)'}
  - {name: batch_size, type: Integer, description: 'Per-device batch size'}
  - {name: grad_accum_steps, type: Integer, description: 'Gradient accumulation steps'}
  - {name: num_generations, type: Integer, description: 'Generations per prompt (4-8)'}
  - {name: temperature, type: Float, description: 'Sampling temperature (0.8-1.0)'}

outputs:
  - {name: grpo_model, type: Model, description: 'GRPO-trained LoRA adapters'}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v24-gpu
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import torch
        import numpy as np
        import pickle
        import json
        import re
        from datasets import Dataset
        from unsloth import FastLanguageModel
        from trl import GRPOConfig, GRPOTrainer
        import argparse
        def check_gpu():
            if torch.cuda.is_available():
                print("=" * 60)
                print("GPU Available")
                for i in range(torch.cuda.device_count()):
                    print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
                    print(f"VRAM: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB")
                print("=" * 60)
            else:
                print("WARNING: No GPU available")
        def main():
            check_gpu()
            parser = argparse.ArgumentParser()
            parser.add_argument('--base_model_path', type=str, required=True)
            parser.add_argument('--lora_adapters_path', type=str, required=True)
            parser.add_argument('--tokenizer_path', type=str, required=True)
            parser.add_argument('--grpo_dataset', type=str, required=True)
            parser.add_argument('--max_seq_length', type=int, required=True)
            parser.add_argument('--lora_rank', type=int, required=True)
            parser.add_argument('--learning_rate', type=float, required=True)
            parser.add_argument('--num_train_steps', type=int, required=True)
            parser.add_argument('--batch_size', type=int, required=True)
            parser.add_argument('--grad_accum_steps', type=int, required=True)
            parser.add_argument('--num_generations', type=int, required=True)
            parser.add_argument('--temperature', type=float, required=True)
            parser.add_argument('--grpo_model', type=str, required=True)
            args = parser.parse_args()
            print("=" * 60)
            print("GRPO Training with Unsloth (Continuing from SFT)")
            print(f"Base Model: {args.base_model_path}")
            print(f"SFT LoRA Adapters: {args.lora_adapters_path}")
            print(f"Tokenizer: {args.tokenizer_path}")
            print(f"Max Seq Length: {args.max_seq_length}")
            print(f"LoRA Rank: {args.lora_rank}")
            print(f"Learning Rate: {args.learning_rate}")
            print(f"Training Steps: {args.num_train_steps}")
            print(f"Num Generations: {args.num_generations}")
            print("=" * 60)
            print("Loading dataset from pickle...")
            with open(args.grpo_dataset, 'rb') as f:
                dataset_dict = pickle.load(f)
            if hasattr(dataset_dict, 'keys') and 'train' in dataset_dict:
                dataset = dataset_dict['train']
            else:
                dataset = dataset_dict
            print(f"Dataset loaded: {len(dataset)} examples")
            print(f"Sample prompt: {dataset[0]['prompt']}")
            print(f"Sample answer: {dataset[0]['answer'][:100]}...")
            print("Loading tokenizer from SFT training...")
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path, trust_remote_code=True)
            print(f"Tokenizer loaded, vocab size: {len(tokenizer)}")
            print("Loading base model with resized embeddings from SFT...")
            model, _ = FastLanguageModel.from_pretrained(
                model_name=args.base_model_path,
                max_seq_length=args.max_seq_length,
                load_in_4bit=True,
                fast_inference=True,
                max_lora_rank=args.lora_rank,
                gpu_memory_utilization=0.9,
            )
            print("Base model loaded successfully")
            sft_adapter_path = os.path.join(args.lora_adapters_path, "adapter")
            if os.path.exists(sft_adapter_path):
                print(f"Loading SFT LoRA adapters from {sft_adapter_path}")
                from peft import PeftModel
                model = PeftModel.from_pretrained(model, sft_adapter_path, is_trainable=True)
                print("SFT LoRA adapters loaded and set to trainable")
            else:
                print(f"ERROR: SFT adapter not found at {sft_adapter_path}")
                print("Creating new LoRA adapters instead...")
                model = FastLanguageModel.get_peft_model(
                    model,
                    r=args.lora_rank,
                    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
                    lora_alpha=args.lora_rank,
                    use_gradient_checkpointing="unsloth",
                    random_state=3407,
                )
                print("New LoRA created")
            print("Defining reward functions...")
            def format_reward_func(completions, **kwargs):
                scores = []
                for completion in completions:
                    score = 0.0
                    response = completion[0]["content"]
                    if response.count("<think>") == 1:
                        score += 0.25
                    if response.count("</think>") == 1:
                        score += 0.25
                    if response.count("<tool_call>") == 1:
                        score += 0.25
                    if response.count("</tool_call>") == 1:
                        score += 0.25
                    scores.append(score)
                return scores
            def json_validity_reward_func(completions, **kwargs):
                scores = []
                for completion in completions:
                    score = 0.0
                    response = completion[0]["content"]
                    try:
                        match = re.search(r"<tool_call>(.*?)</tool_call>", response, re.DOTALL)
                        if match:
                            json.loads(match.group(1).strip())
                            score += 1.0
                    except:
                        score -= 0.5
                    scores.append(score)
                return scores
            global PRINT_COUNTER
            PRINT_COUNTER = 0
            def answer_similarity_reward_func(prompts, completions, answer, **kwargs):
                global PRINT_COUNTER
                if PRINT_COUNTER % 5 == 0:
                    q = prompts[0][-1]["content"] if isinstance(prompts[0], list) else prompts[0]
                    response = completions[0][0]["content"]
                    print("-" * 60)
                    print(f"Question: {q[:100]}...")
                    print(f"Answer: {answer[0][:100]}...")
                    print(f"Response: {response[:200]}...")
                    print("-" * 60)
                PRINT_COUNTER += 1
                scores = []
                for completion, true_answer in zip(completions, answer):
                    response = completion[0]["content"]
                    if true_answer in response:
                        scores.append(2.0)
                    else:
                        scores.append(0.0)
                return scores
            max_prompt_length = 512
            print("=" * 60)
            print("GRPO Training Configuration:")
            print(f"Max prompt length: {max_prompt_length}")
            print(f"Max completion length: {args.max_seq_length - max_prompt_length}")
            print(f"Effective batch size: {args.batch_size * args.grad_accum_steps}")
            print("=" * 60)
            training_args = GRPOConfig(
                learning_rate=args.learning_rate,
                adam_beta1=0.9,
                adam_beta2=0.99,
                weight_decay=0.1,
                warmup_ratio=0.1,
                lr_scheduler_type="cosine",
                optim="paged_adamw_8bit",
                logging_steps=1,
                per_device_train_batch_size=args.batch_size,
                gradient_accumulation_steps=args.grad_accum_steps,
                num_generations=args.num_generations,
                max_prompt_length=max_prompt_length,
                max_completion_length=args.max_seq_length - max_prompt_length,
                max_steps=args.num_train_steps,
                save_steps=args.num_train_steps,
                max_grad_norm=0.1,
                report_to="none",
                output_dir="outputs",
                temperature=args.temperature,
            )
            print("Initializing GRPO Trainer...")
            trainer = GRPOTrainer(
                model=model,
                processing_class=tokenizer,
                reward_funcs=[format_reward_func, json_validity_reward_func, answer_similarity_reward_func],
                args=training_args,
                train_dataset=dataset,
            )
            print("Starting GRPO training...")
            trainer.train()
            os.makedirs(args.grpo_model, exist_ok=True)
            lora_save_path = os.path.join(args.grpo_model, "grpo_lora")
            print(f"Saving LoRA adapters to {lora_save_path}")
            model.save_lora(lora_save_path)
            print(f"Saving tokenizer to {args.grpo_model}")
            tokenizer.save_pretrained(args.grpo_model)
            training_info = {
                "base_model": args.base_model_name,
                "training_steps": args.num_train_steps,
                "learning_rate": args.learning_rate,
                "lora_rank": args.lora_rank,
                "num_generations": args.num_generations,
                "final_dataset_size": len(dataset),
                "max_prompt_length": max_prompt_length,
                "max_completion_length": args.max_seq_length - max_prompt_length,
            }
            with open(os.path.join(args.grpo_model, "grpo_info.json"), "w") as f:
                json.dump(training_info, f, indent=2)
            print("=" * 60)
            print("GRPO training complete!")
            print(f"LoRA saved to: {lora_save_path}")
            print(f"Tokenizer saved to: {args.grpo_model}")
            print("=" * 60)
        if __name__ == '__main__':
            main()
    args:
      - --base_model_path
      - {inputPath: base_model_path}
      - --lora_adapters_path
      - {inputPath: lora_adapters_path}
      - --tokenizer_path
      - {inputPath: tokenizer_path}
      - --grpo_dataset
      - {inputPath: grpo_dataset}
      - --max_seq_length
      - {inputValue: max_seq_length}
      - --lora_rank
      - {inputValue: lora_rank}
      - --learning_rate
      - {inputValue: learning_rate}
      - --num_train_steps
      - {inputValue: num_train_steps}
      - --batch_size
      - {inputValue: batch_size}
      - --grad_accum_steps
      - {inputValue: grad_accum_steps}
      - --num_generations
      - {inputValue: num_generations}
      - --temperature
      - {inputValue: temperature}
      - --grpo_model
      - {outputPath: grpo_model}
