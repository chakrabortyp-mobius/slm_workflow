name: GRPO Training
description: Continues training from SFT LoRA using GRPO with Hugging Face Transformers

inputs:
  - {name: base_model_path, type: Model, description: 'Base model with resized embeddings from SFT'}
  - {name: lora_adapters_path, type: Model, description: 'SFT LoRA adapters to continue from'}
  - {name: tokenizer_path, type: Model, description: 'Tokenizer with special tokens from SFT'}
  - {name: grpo_dataset, type: Dataset, description: 'Pickle dataset with prompt and answer'}
  - {name: max_seq_length, type: Integer, description: 'Maximum sequence length (e.g., 2048)'}
  - {name: learning_rate, type: Float, description: 'Learning rate (e.g., 5e-6)'}
  - {name: num_train_steps, type: Integer, description: 'Training steps (e.g., 100-500)'}
  - {name: batch_size, type: Integer, description: 'Per-device batch size'}
  - {name: grad_accum_steps, type: Integer, description: 'Gradient accumulation steps'}
  - {name: num_generations, type: Integer, description: 'Generations per prompt (4-8)'}
  - {name: temperature, type: Float, description: 'Sampling temperature (0.8-1.0)'}

outputs:
  - {name: grpo_model, type: Model, description: 'GRPO-trained LoRA adapters'}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v24-gpu
    command:
      - sh
      - -c
      - (pip install --no-cache-dir trl vllm > /dev/null 2>&1) && "$0" "$@"
      - python3
      - -u
      - -c
      - |
        def _make_parent_dirs_and_return_path(file_path):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path
        def train_grpo(
            base_model_path,
            lora_adapters_path,
            tokenizer_path,
            grpo_dataset_path,
            max_seq_length_val,
            learning_rate_val,
            num_train_steps_val,
            batch_size_val,
            grad_accum_steps_val,
            num_generations_val,
            temperature_val,
            grpo_model_path,
        ):
            import os
            import torch
            import numpy as np
            import pickle
            import json
            import re
            from transformers import AutoTokenizer, AutoModelForCausalLM
            from peft import PeftModel
            from trl import GRPOConfig, GRPOTrainer
            from vllm import SamplingParams
            import logging
            logging.basicConfig(level=logging.INFO)
            logger = logging.getLogger("grpo_training")
            def check_gpu():
                if torch.cuda.is_available():
                    logger.info("=" * 60)
                    logger.info("GPU Available")
                    for i in range(torch.cuda.device_count()):
                        logger.info(f"GPU {i}: {torch.cuda.get_device_name(i)}")
                        logger.info(f"VRAM: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB")
                    logger.info("=" * 60)
                else:
                    logger.warning("WARNING: No GPU available")
            check_gpu()
            logger.info("=" * 60)
            logger.info("GRPO Training (Continuing from SFT)")
            logger.info(f"Base Model: {base_model_path}")
            logger.info(f"SFT LoRA: {lora_adapters_path}")
            logger.info(f"Tokenizer: {tokenizer_path}")
            logger.info(f"Max Seq Length: {max_seq_length_val}")
            logger.info(f"Learning Rate: {learning_rate_val}")
            logger.info(f"Training Steps: {num_train_steps_val}")
            logger.info(f"Num Generations: {num_generations_val}")
            logger.info("=" * 60)
            logger.info("Loading dataset from pickle...")
            with open(grpo_dataset_path, 'rb') as f:
                dataset_dict = pickle.load(f)
            if hasattr(dataset_dict, 'keys') and 'train' in dataset_dict:
                dataset = dataset_dict['train']
            else:
                dataset = dataset_dict
            logger.info(f"Dataset loaded: {len(dataset)} examples")
            if len(dataset) > 0:
                logger.info(f"Sample prompt: {dataset[0]['prompt']}")
                logger.info(f"Sample answer: {dataset[0]['answer'][:100]}...")
            logger.info("Loading tokenizer from SFT training...")
            tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
            logger.info(f"Tokenizer loaded, vocab size: {len(tokenizer)}")
            logger.info("Loading base model with resized embeddings...")
            model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch.bfloat16,
                device_map="auto",
                trust_remote_code=True
            )
            logger.info("Base model loaded successfully")
            sft_adapter_path = os.path.join(lora_adapters_path, "adapter")
            if os.path.exists(sft_adapter_path):
                logger.info(f"Loading SFT LoRA adapters from {sft_adapter_path}")
                model = PeftModel.from_pretrained(model, sft_adapter_path, is_trainable=True)
                logger.info("SFT LoRA adapters loaded and set to trainable")
            else:
                logger.error(f"ERROR: SFT adapter not found at {sft_adapter_path}")
                raise FileNotFoundError(f"LoRA adapters not found at {sft_adapter_path}")
            logger.info("Defining reward functions...")
            def format_reward_func(completions, **kwargs):
                scores = []
                for completion in completions:
                    score = 0.0
                    response = completion[0]["content"]
                    if response.count("<think>") == 1:
                        score += 0.25
                    if response.count("</think>") == 1:
                        score += 0.25
                    if response.count("<tool_call>") == 1:
                        score += 0.25
                    if response.count("</tool_call>") == 1:
                        score += 0.25
                    scores.append(score)
                return scores
            def json_validity_reward_func(completions, **kwargs):
                scores = []
                for completion in completions:
                    score = 0.0
                    response = completion[0]["content"]
                    try:
                        match = re.search(r"<tool_call>(.*?)</tool_call>", response, re.DOTALL)
                        if match:
                            json.loads(match.group(1).strip())
                            score += 1.0
                    except:
                        score -= 0.5
                    scores.append(score)
                return scores
            global PRINT_COUNTER
            PRINT_COUNTER = 0
            def answer_similarity_reward_func(prompts, completions, answer, **kwargs):
                global PRINT_COUNTER
                if PRINT_COUNTER % 5 == 0:
                    q = prompts[0][-1]["content"] if isinstance(prompts[0], list) else str(prompts[0])[:100]
                    response = completions[0][0]["content"]
                    logger.info("-" * 60)
                    logger.info(f"Question: {q}...")
                    logger.info(f"Answer: {answer[0][:100]}...")
                    logger.info(f"Response: {response[:200]}...")
                    logger.info("-" * 60)
                PRINT_COUNTER += 1
                scores = []
                for completion, true_answer in zip(completions, answer):
                    response = completion[0]["content"]
                    if true_answer in response:
                        scores.append(2.0)
                    else:
                        scores.append(0.0)
                return scores
            max_prompt_length = 512
            logger.info("=" * 60)
            logger.info("GRPO Training Configuration:")
            logger.info(f"Max prompt length: {max_prompt_length}")
            logger.info(f"Max completion length: {max_seq_length_val - max_prompt_length}")
            logger.info(f"Effective batch size: {batch_size_val * grad_accum_steps_val}")
            logger.info("=" * 60)
            vllm_sampling_params = SamplingParams(
                min_p=0.1,
                top_p=1.0,
                top_k=-1,
                seed=3407,
                stop=[tokenizer.eos_token],
                include_stop_str_in_output=True,
            )
            training_args = GRPOConfig(
                vllm_sampling_params=vllm_sampling_params,
                learning_rate=learning_rate_val,
                adam_beta1=0.9,
                adam_beta2=0.99,
                weight_decay=0.1,
                warmup_ratio=0.1,
                lr_scheduler_type="cosine",
                optim="paged_adamw_8bit",
                logging_steps=1,
                per_device_train_batch_size=batch_size_val,
                gradient_accumulation_steps=grad_accum_steps_val,
                num_generations=num_generations_val,
                max_prompt_length=max_prompt_length,
                max_completion_length=max_seq_length_val - max_prompt_length,
                max_steps=num_train_steps_val,
                save_steps=num_train_steps_val,
                max_grad_norm=0.1,
                report_to="none",
                output_dir="outputs",
                temperature=temperature_val,
            )
            logger.info("Initializing GRPO Trainer...")
            trainer = GRPOTrainer(
                model=model,
                processing_class=tokenizer,
                reward_funcs=[format_reward_func, json_validity_reward_func, answer_similarity_reward_func],
                args=training_args,
                train_dataset=dataset,
            )
            logger.info("Starting GRPO training...")
            trainer.train()
            os.makedirs(grpo_model_path, exist_ok=True)
            adapter_save_path = os.path.join(grpo_model_path, "adapter")
            os.makedirs(adapter_save_path, exist_ok=True)
            logger.info(f"Saving GRPO-trained LoRA adapters to {adapter_save_path}")
            model.save_pretrained(adapter_save_path)
            logger.info(f"Saving tokenizer to {grpo_model_path}")
            tokenizer.save_pretrained(grpo_model_path)
            training_info = {
                "base_model": base_model_path,
                "training_steps": num_train_steps_val,
                "learning_rate": learning_rate_val,
                "num_generations": num_generations_val,
                "final_dataset_size": len(dataset),
                "max_prompt_length": max_prompt_length,
                "max_completion_length": max_seq_length_val - max_prompt_length,
            }
            with open(os.path.join(grpo_model_path, "grpo_info.json"), "w") as f:
                json.dump(training_info, f, indent=2)
            logger.info("=" * 60)
            logger.info("GRPO training complete!")
            logger.info(f"LoRA adapters saved to: {adapter_save_path}")
            logger.info(f"Tokenizer saved to: {grpo_model_path}")
            logger.info("=" * 60)
        import argparse
        _parser = argparse.ArgumentParser(prog="GRPO Training", description="GRPO training for LoRA models")
        _parser.add_argument("--base_model_path", type=str, required=True)
        _parser.add_argument("--lora_adapters_path", type=str, required=True)
        _parser.add_argument("--tokenizer_path", type=str, required=True)
        _parser.add_argument("--grpo_dataset", dest="grpo_dataset_path", type=str, required=True)
        _parser.add_argument("--max_seq_length", dest="max_seq_length_val", type=int, required=True)
        _parser.add_argument("--learning_rate", dest="learning_rate_val", type=float, required=True)
        _parser.add_argument("--num_train_steps", dest="num_train_steps_val", type=int, required=True)
        _parser.add_argument("--batch_size", dest="batch_size_val", type=int, required=True)
        _parser.add_argument("--grad_accum_steps", dest="grad_accum_steps_val", type=int, required=True)
        _parser.add_argument("--num_generations", dest="num_generations_val", type=int, required=True)
        _parser.add_argument("--temperature", dest="temperature_val", type=float, required=True)
        _parser.add_argument("--grpo_model", dest="grpo_model_path", type=_make_parent_dirs_and_return_path, required=True)
        _parsed_args = vars(_parser.parse_args())
        train_grpo(**_parsed_args)
    args:
      - --base_model_path
      - {inputPath: base_model_path}
      - --lora_adapters_path
      - {inputPath: lora_adapters_path}
      - --tokenizer_path
      - {inputPath: tokenizer_path}
      - --grpo_dataset
      - {inputPath: grpo_dataset}
      - --max_seq_length
      - {inputValue: max_seq_length}
      - --learning_rate
      - {inputValue: learning_rate}
      - --num_train_steps
      - {inputValue: num_train_steps}
      - --batch_size
      - {inputValue: batch_size}
      - --grad_accum_steps
      - {inputValue: grad_accum_steps}
      - --num_generations
      - {inputValue: num_generations}
      - --temperature
      - {inputValue: temperature}
      - --grpo_model
      - {outputPath: grpo_model}
