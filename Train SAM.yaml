name: Train UNet Model
description: Trains UNet model using dataset directory from CDN data loader brick
inputs:
  - name: dataset_dir
    type: String
  - name: learning_rate
    type: Float
  - name: batch_size
    type: Integer
  - name: num_epochs
    type: Integer
  - name: num_workers
    type: Integer
  - name: image_height
    type: Integer
  - name: image_width
    type: Integer
    
outputs:
  - name: model_checkpoint
    type: String
    description: Path to saved model checkpoint
  - name: training_metrics
    type: String
    description: JSON file with training metrics
  - name: saved_images_dir
    type: String
    description: Directory with sample prediction images
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - sh
      - -c
      - |
        set -e
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import argparse
        import os
        import json
        import torch
        import torch.nn as nn
        import torch.optim as optim
        import albumentations as A
        from albumentations.pytorch import ToTensorV2
        from tqdm import tqdm
        from PIL import Image
        import numpy as np
        from torch.utils.data import Dataset, DataLoader
        import torchvision
        import torchvision.transforms.functional as TF
        
        # ==================== DATASET CLASS ====================
        class CarvanaDataset(Dataset):
            def __init__(self, image_dir, mask_dir, transform=None):
                self.image_dir = image_dir
                self.mask_dir = mask_dir
                self.transform = transform
                self.images = os.listdir(image_dir)
            
            def __len__(self):
                return len(self.images)
            
            def __getitem__(self, index):
                img_path = os.path.join(self.image_dir, self.images[index])
                mask_path = os.path.join(self.mask_dir, self.images[index].replace(".jpg", ".png"))
                if not os.path.exists(mask_path):
                    mask_path = os.path.join(self.mask_dir, self.images[index])
                
                image = np.array(Image.open(img_path).convert("RGB"))
                mask = np.array(Image.open(mask_path).convert("L"), dtype=np.float32)
                mask[mask == 255.0] = 1.0
                
                if self.transform is not None:
                    augmentations = self.transform(image=image, mask=mask)
                    image = augmentations["image"]
                    mask = augmentations["mask"]
                
                return image, mask
        
        # ==================== UTILITY FUNCTIONS ====================
        def save_checkpoint(state, filename="my_checkpoint.pth.tar"):
            print("=> Saving checkpoint")
            torch.save(state, filename)
        
        def load_checkpoint(checkpoint, model):
            print("Loading checkpoint")
            model.load_state_dict(checkpoint["state_dict"])
        
        def get_loaders(
            train_dir,
            train_maskdir,
            val_dir,
            val_maskdir,
            batch_size,
            train_transform,
            val_transform,
            num_workers=4,
            pin_memory=True,
        ):
            train_ds = CarvanaDataset(
                image_dir=train_dir,
                mask_dir=train_maskdir,
                transform=train_transform,
            )
            train_loader = DataLoader(
                train_ds,
                batch_size=batch_size,
                num_workers=num_workers,
                pin_memory=pin_memory,
                shuffle=True,
            )
            
            val_ds = CarvanaDataset(
                image_dir=val_dir,
                mask_dir=val_maskdir,
                transform=val_transform,
            )
            val_loader = DataLoader(
                val_ds,
                batch_size=batch_size,
                num_workers=num_workers,
                pin_memory=pin_memory,
                shuffle=False,
            )
            
            return train_loader, val_loader
        
        def check_accuracy(loader, model, device="cuda"):
            num_correct = 0
            num_pixels = 0
            dice_score = 0
            model.eval()
            
            with torch.no_grad():
                for x, y in loader:
                    x = x.to(device)
                    y = y.to(device).unsqueeze(1)
                    preds = torch.sigmoid(model(x))
                    preds = (preds > 0.5).float()
                    num_correct += (preds == y).sum()
                    num_pixels += torch.numel(preds)
                    dice_score += (2 * (preds * y).sum()) / (
                        (preds + y).sum() + 1e-8
                    )
            
            accuracy = float(num_correct/num_pixels*100)
            dice = float(dice_score/len(loader))
            print(f"Got {num_correct}/{num_pixels} with acc {accuracy:.2f}")
            print(f"Dice score: {dice}")
            model.train()
            
            return accuracy, dice
        
        def save_predictions_as_imgs(
            loader, model, folder="saved_images/", device="cuda"
        ):
            model.eval()
            for idx, (x, y) in enumerate(loader):
                x = x.to(device=device)
                with torch.no_grad():
                    preds = torch.sigmoid(model(x))
                    preds = (preds > 0.5).float()
                torchvision.utils.save_image(
                    preds, f"{folder}/pred_{idx}.png"
                )
                torchvision.utils.save_image(y.unsqueeze(1), f"{folder}{idx}.png")
            model.train()
        
        # ==================== MODEL CLASSES ====================
        class DoubleConv(nn.Module):
            def __init__(self, in_channels, out_channels):
                super(DoubleConv, self).__init__()
                self.conv = nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(inplace=True),
                    nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),
                    nn.BatchNorm2d(out_channels),
                    nn.ReLU(inplace=True),
                )
            
            def forward(self, x):
                return self.conv(x)
        
        class UNET(nn.Module):
            def __init__(
                    self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],
            ):
                super(UNET, self).__init__()
                self.ups = nn.ModuleList()
                self.downs = nn.ModuleList()
                self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
                
                # Down part of UNET
                for feature in features:
                    self.downs.append(DoubleConv(in_channels, feature))
                    in_channels = feature
                
                # Up part of UNET
                for feature in reversed(features):
                    self.ups.append(
                        nn.ConvTranspose2d(
                            feature*2, feature, kernel_size=2, stride=2,
                        )
                    )
                    self.ups.append(DoubleConv(feature*2, feature))
                
                self.bottleneck = DoubleConv(features[-1], features[-1]*2)
                self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)
            
            def forward(self, x):
                skip_connections = []
                
                for down in self.downs:
                    x = down(x)
                    skip_connections.append(x)
                    x = self.pool(x)
                
                x = self.bottleneck(x)
                skip_connections = skip_connections[::-1]
                
                for idx in range(0, len(self.ups), 2):
                    x = self.ups[idx](x)
                    skip_connection = skip_connections[idx//2]
                    
                    if x.shape != skip_connection.shape:
                        x = TF.resize(x, size=skip_connection.shape[2:])
                    
                    concat_skip = torch.cat((skip_connection, x), dim=1)
                    x = self.ups[idx+1](concat_skip)
                
                return self.final_conv(x)
        
        # ==================== TRAINING FUNCTION ====================
        def train_fn(loader, model, optimizer, loss_fn, scaler):
            loop = tqdm(loader)
            
            for batch_idx, (data, targets) in enumerate(loop):
                data = data.to(device=DEVICE)
                targets = targets.float().unsqueeze(1).to(device=DEVICE)

                with torch.cuda.amp.autocast():
                    predictions = model(data)
                    loss = loss_fn(predictions, targets)

                optimizer.zero_grad()
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()

                loop.set_postfix(loss=loss.item())
        
        # ==================== MAIN TRAINING ====================
        parser = argparse.ArgumentParser()
        parser.add_argument('--dataset_dir', type=str, required=True)
        parser.add_argument('--learning_rate', type=float, default=1e-4)
        parser.add_argument('--batch_size', type=int, default=16)
        parser.add_argument('--num_epochs', type=int, default=3)
        parser.add_argument('--num_workers', type=int, default=2)
        parser.add_argument('--image_height', type=int, default=160)
        parser.add_argument('--image_width', type=int, default=240)
        parser.add_argument('--model_checkpoint', type=str, required=True)
        parser.add_argument('--training_metrics', type=str, required=True)
        parser.add_argument('--saved_images_dir', type=str, required=True)
        args = parser.parse_args()
        
        # Hyperparameters
        LEARNING_RATE = args.learning_rate
        DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
        BATCH_SIZE = args.batch_size
        NUM_EPOCHS = args.num_epochs
        NUM_WORKERS = args.num_workers
        IMAGE_HEIGHT = args.image_height
        IMAGE_WIDTH = args.image_width
        PIN_MEMORY = True
        LOAD_MODEL = False
        
        # Set up directories from CDN loader brick output
        TRAIN_IMG_DIR = os.path.join(args.dataset_dir, "train_images")
        TRAIN_MASK_DIR = os.path.join(args.dataset_dir, "train_masks")
        VAL_IMG_DIR = os.path.join(args.dataset_dir, "test_images")
        VAL_MASK_DIR = os.path.join(args.dataset_dir, "test_masks")
        
        print(f"Dataset directory: {args.dataset_dir}")
        print(f"Train images: {TRAIN_IMG_DIR}")
        print(f"Train masks: {TRAIN_MASK_DIR}")
        print(f"Val images: {VAL_IMG_DIR}")
        print(f"Val masks: {VAL_MASK_DIR}")
        print(f"Device: {DEVICE}")
        
        # Transforms
        train_transform = A.Compose(
            [
                A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),
                A.Rotate(limit=35, p=1.0),
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.1),
                A.Normalize(
                    mean=[0.0, 0.0, 0.0],
                    std=[1.0, 1.0, 1.0],
                    max_pixel_value=255.0,
                ),
                ToTensorV2(),
            ],
        )
        
        val_transforms = A.Compose(
            [
                A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),
                A.Normalize(
                    mean=[0.0, 0.0, 0.0],
                    std=[1.0, 1.0, 1.0],
                    max_pixel_value=255.0,
                ),
                ToTensorV2(),
            ],
        )
        
        # Model, loss, optimizer
        model = UNET(in_channels=3, out_channels=1).to(DEVICE)
        loss_fn = nn.BCEWithLogitsLoss()
        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
        
        # Get data loaders
        train_loader, val_loader = get_loaders(
            TRAIN_IMG_DIR,
            TRAIN_MASK_DIR,
            VAL_IMG_DIR,
            VAL_MASK_DIR,
            BATCH_SIZE,
            train_transform,
            val_transforms,
            NUM_WORKERS,
            PIN_MEMORY,
        )
        
        if LOAD_MODEL:
            load_checkpoint(torch.load("my_checkpoint.pth.tar"), model)
        
        print(f"Starting training for {NUM_EPOCHS} epochs...")
        check_accuracy(val_loader, model, device=DEVICE)
        scaler = torch.cuda.amp.GradScaler()

        metrics = {
            "epochs": [],
            "val_accuracy": [],
            "val_dice": []
        }

        for epoch in range(NUM_EPOCHS):
            print(f"Epoch {epoch+1}/{NUM_EPOCHS}")
            train_fn(train_loader, model, optimizer, loss_fn, scaler)
            
            # save model
            checkpoint = {
                "state_dict": model.state_dict(),
                "optimizer": optimizer.state_dict(),
            }
            
            os.makedirs(os.path.dirname(args.model_checkpoint), exist_ok=True)
            save_checkpoint(checkpoint, args.model_checkpoint)

            val_acc, val_dice = check_accuracy(val_loader, model, device=DEVICE)

            metrics["epochs"].append(epoch + 1)
            metrics["val_accuracy"].append(val_acc)
            metrics["val_dice"].append(val_dice)
            

            os.makedirs(args.saved_images_dir, exist_ok=True)
            save_predictions_as_imgs(
                val_loader, model, folder=args.saved_images_dir, device=DEVICE
            )
        
        # Save training metrics
        os.makedirs(os.path.dirname(args.training_metrics), exist_ok=True)
        with open(args.training_metrics, 'w') as f:
            json.dump(metrics, f, indent=2)
        
        print(f"Training completed!")
        print(f"Model checkpoint saved: {args.model_checkpoint}")
        print(f"Training metrics saved: {args.training_metrics}")
        print(f"Sample images saved: {args.saved_images_dir}")
    args:
      - --dataset_dir
      - {inputPath: dataset_dir}
      - --learning_rate
      - {inputValue: learning_rate}
      - --batch_size
      - {inputValue: batch_size}
      - --num_epochs
      - {inputValue: num_epochs}
      - --num_workers
      - {inputValue: num_workers}
      - --image_height
      - {inputValue: image_height}
      - --image_width
      - {inputValue: image_width}
      - --model_checkpoint
      - {outputPath: model_checkpoint}
      - --training_metrics
      - {outputPath: training_metrics}
      - --saved_images_dir
      - {outputPath: saved_images_dir}
