name: Train SAM
description: Fine-tunes a Gemma3 model using LoRA on a dataset with ChatML formatting.

inputs:
  - name: tokenizer_dir
    type: Model
  - name: gemma3_config_json
    type: Data
  - name: gemma3_model_py
    type: Data
  - name: best_pt
    type: Model
  - name: dataset_name
    type: String
    default: "Jofthomas/hermes-function-calling-thinking-V1"
  - name: lora_rank
    type: Integer
    default: "128"
  - name: lora_alpha
    type: Integer
    default: "64"
  - name: lora_dropout
    type: Float
    default: "0.2"
  - name: num_train_epochs
    type: Integer
    default: "20"
  - name: learning_rate
    type: Float
    default: "0.0001"
  - name: batch_size
    type: Integer
    default: "1"
  - name: gradient_accumulation_steps
    type: Integer
    default: "4"
  - name: sliding_window
    type: Integer
    default: "2048"

outputs:
  - name: trained_model
    type: Model
  - name: training_report
    type: Data

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - bash
      - -ec
      - |-
        set -o pipefail
        
        cat >/tmp/train_lora.py <<'PY'
        import argparse, json, os, shutil, importlib.util
        import torch
        import torch.nn as nn
        from transformers import PreTrainedTokenizerFast, PretrainedConfig, PreTrainedModel
        from transformers.modeling_outputs import CausalLMOutputWithPast
        from datasets import load_dataset
        from peft import LoraConfig, TaskType
        from trl import SFTTrainer, SFTConfig
        from types import MethodType

        # --------- dynamic loader ------------
        def _load_model_class_from_file(py_path: str, class_name: str = "Gemma3Model"):
            if not py_path.endswith(".py"):
                tmp = py_path + ".py"
                shutil.copyfile(py_path, tmp)
                py_path = tmp
            spec = importlib.util.spec_from_file_location("gemma3_model", py_path)
            if spec is None or spec.loader is None:
                raise ImportError(f"Could not load spec from {py_path}")
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)
            if not hasattr(mod, class_name):
                raise AttributeError(f"{py_path} does not define {class_name}")
            return getattr(mod, class_name)

        def _patch_resize_token_embeddings(Gemma3Model):
            def resize_token_embeddings(self, new_size):
                old_emb = self.token_emb
                old_out = self.out_head
                d = old_emb.embedding_dim
                device, dtype = old_emb.weight.device, old_emb.weight.dtype

                new_emb = nn.Embedding(new_size, d, device=device, dtype=dtype)
                num_copy = min(old_emb.num_embeddings, new_size)
                new_emb.weight.data[:num_copy] = old_emb.weight.data[:num_copy]
                self.token_emb = new_emb

                new_out = nn.Linear(d, new_size, bias=False, device=device, dtype=dtype)
                new_out.weight.data[:num_copy] = old_out.weight.data[:num_copy]
                self.out_head = new_out
                self.cfg["vocab_size"] = int(new_size)
                return self.token_emb
            Gemma3Model.resize_token_embeddings = resize_token_embeddings

        # --------- config wrapper ------------
        class Gemma3Config(PretrainedConfig):
            model_type = "gemma3"
            def __init__(self, **kwargs):
                super().__init__(**kwargs)

        # --------- model wrapper ------------
        class Gemma3ForCausalLM(PreTrainedModel):
            config_class = Gemma3Config
            base_model_prefix = "gemma3"

            def __init__(self, config: Gemma3Config):
                super().__init__(config)
                self.model = None

            @classmethod
            def from_pretrained(cls, cfg_path, model_py_path, weights_path, device):
                cfg_dict = json.load(open(cfg_path))
                cfg_dict["dtype"] = torch.float16 if str(cfg_dict.get("dtype")) == "float16" else torch.float32
                config = Gemma3Config(**cfg_dict)

                ModelClass = _load_model_class_from_file(model_py_path, "Gemma3Model")
                _patch_resize_token_embeddings(ModelClass)
                model = ModelClass(cfg_dict).to(device)
                state = torch.load(weights_path, map_location=device)
                model.load_state_dict(state, strict=True)

                wrapper = cls(config)
                wrapper.model = model
                return wrapper

            def forward(self, input_ids=None, labels=None, **kwargs):
                logits, loss = self.model(input_ids=input_ids, labels=labels)
                return CausalLMOutputWithPast(loss=loss, logits=logits)

            def resize_token_embeddings(self, new_size):
                return self.model.resize_token_embeddings(new_size)
            
            def prepare_inputs_for_generation(self, input_ids, **kwargs):
                return {"input_ids": input_ids, **kwargs}

        def extend_sliding_rope(self, new_len: int):
            device = next(self.parameters()).device
            cos, sin = self.build_rope_cache(
                seq_len=new_len,
                head_dim=self.cfg["head_dim"],
                device=device,
                dtype=torch.float32,
            )
            self.cos_local, self.sin_local = cos, sin
            self.cfg["sliding_window"] = int(new_len)
            print(f"[INFO] sliding_window -> {new_len}")

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--tokenizer-dir", required=True)
            ap.add_argument("--gemma3-config-json", required=True)
            ap.add_argument("--gemma3-model-py", required=True)
            ap.add_argument("--best-pt", required=True)
            ap.add_argument("--dataset-name", required=True)
            ap.add_argument("--lora-rank", type=int, required=True)
            ap.add_argument("--lora-alpha", type=int, required=True)
            ap.add_argument("--lora-dropout", type=float, required=True)
            ap.add_argument("--num-train-epochs", type=int, required=True)
            ap.add_argument("--learning-rate", type=float, required=True)
            ap.add_argument("--batch-size", type=int, required=True)
            ap.add_argument("--gradient-accumulation-steps", type=int, required=True)
            ap.add_argument("--sliding-window", type=int, required=True)
            ap.add_argument("--trained-model", required=True)
            ap.add_argument("--training-report", required=True)
            a = ap.parse_args()

            device = "cuda" if torch.cuda.is_available() else "cpu"
            print(f"[INFO] Using device: {device}")

            # Load tokenizer
            print(f"[INFO] Loading tokenizer from {a.tokenizer_dir}")
            tok = PreTrainedTokenizerFast.from_pretrained(a.tokenizer_dir)
            
            # Set chat template
            tok.chat_template = (
                "{{ bos_token }}"
                "{% if messages[0]['role'] == 'system' %}"
                "{{ raise_exception('System role not supported') }}"
                "{% endif %}"
                "{% for message in messages %}"
                "{{ '<start_of_turn>' ~ message['role'] ~ '\\n' ~ (message['content'] | trim) "
                "~ '<end_of_turn>' ~ eos_token ~ '\\n' }}"
                "{% endfor %}"
                "{% if add_generation_prompt %}{{ '<start_of_turn>assistant\\n' }}{% endif %}"
            )

            # Preprocess function
            def preprocess(sample):
                messages = sample["messages"]
                first_message = messages[0]
                if first_message["role"] == "system":
                    system_message_content = first_message["content"]
                    messages[1]["content"] = system_message_content + "Also, before making a call to a function take the time to plan the function to take. Make that thinking process between <think>{your thoughts}</think>\\n\\n" + messages[1]["content"]
                    messages.pop(0)
                return {"text": tok.apply_chat_template(messages, tokenize=False)}

            # Load and prepare dataset
            print(f"[INFO] Loading dataset: {a.dataset_name}")
            dataset = load_dataset(a.dataset_name)
            dataset = dataset.rename_column("conversations", "messages")
            dataset = dataset.map(preprocess, remove_columns="messages")
            dataset = dataset["train"].train_test_split(0.1)
            
            # Select subset of data
            dataset["train"] = dataset["train"].select(range(100))
            dataset["test"] = dataset["test"].select(range(10))
            
            print(f"[INFO] Dataset prepared - train: {len(dataset['train'])}, test: {len(dataset['test'])}")

            # Load model
            print(f"[INFO] Loading model")
            model = Gemma3ForCausalLM.from_pretrained(
                a.gemma3_config_json,
                a.gemma3_model_py,
                a.best_pt,
                device
            )

            # Extend sliding window
            base = model.model
            base.extend_sliding_rope = MethodType(extend_sliding_rope, base)
            base.extend_sliding_rope(a.sliding_window)

            # Resize token embeddings
            print(f"[INFO] Original vocab size: {model.model.token_emb.num_embeddings}")
            model.resize_token_embeddings(len(tok))
            print(f"[INFO] New vocab size: {model.model.token_emb.num_embeddings}")

            model.model.to(torch.float32)

            # LoRA configuration
            target_modules = ["W_query", "W_key", "W_value", "out_proj"]
            peft_config = LoraConfig(
                r=a.lora_rank,
                lora_alpha=a.lora_alpha,
                lora_dropout=a.lora_dropout,
                target_modules=target_modules,
                bias="none",
                task_type=TaskType.CAUSAL_LM
            )
            print(f"[INFO] LoRA config: rank={a.lora_rank}, alpha={a.lora_alpha}, dropout={a.lora_dropout}")

            # Training configuration
            training_arguments = SFTConfig(
                output_dir=a.trained_model,
                per_device_train_batch_size=a.batch_size,
                per_device_eval_batch_size=a.batch_size,
                gradient_accumulation_steps=a.gradient_accumulation_steps,
                save_strategy="no",
                eval_strategy="epoch",
                logging_steps=5,
                learning_rate=a.learning_rate,
                max_grad_norm=1.0,
                weight_decay=0.1,
                warmup_ratio=0.1,
                lr_scheduler_type="cosine",
                report_to="tensorboard",
                bf16=torch.cuda.is_available(),
                hub_private_repo=False,
                push_to_hub=False,
                num_train_epochs=a.num_train_epochs,
                gradient_checkpointing=False,
                packing=False,
            )

            # Trainer setup
            print(f"[INFO] Setting up trainer")
            trainer = SFTTrainer(
                model=model,
                args=training_arguments,
                train_dataset=dataset["train"],
                eval_dataset=dataset["test"],
                processing_class=tok,
                peft_config=peft_config,
            )
            trainer.model.print_trainable_parameters()

            # Train
            print(f"[INFO] Starting training for {a.num_train_epochs} epochs")
            trainer.train()

            # Save model
            os.makedirs(a.trained_model, exist_ok=True)
            trainer.model.save_pretrained(a.trained_model)
            print(f"[SUCCESS] Model saved to {a.trained_model}")

            # Training report
            report = {
                "dataset_name": a.dataset_name,
                "train_samples": len(dataset["train"]),
                "test_samples": len(dataset["test"]),
                "tokenizer_vocab_size": len(tok),
                "lora_config": {
                    "rank": a.lora_rank,
                    "alpha": a.lora_alpha,
                    "dropout": a.lora_dropout,
                    "target_modules": target_modules
                },
                "training_config": {
                    "num_epochs": a.num_train_epochs,
                    "learning_rate": a.learning_rate,
                    "batch_size": a.batch_size,
                    "gradient_accumulation_steps": a.gradient_accumulation_steps,
                    "sliding_window": a.sliding_window
                },
                "device": device,
                "output_dir": a.trained_model
            }
            
            os.makedirs(os.path.dirname(a.training_report) or ".", exist_ok=True)
            with open(a.training_report, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2)
            
            print(f"[SUCCESS] Training report saved to {a.training_report}")

        if __name__ == "__main__":
            main()
        PY

        python3 -u /tmp/train_lora.py "$0" "$@"
    args:
      - --tokenizer-dir
      - {inputPath: tokenizer_dir}
      - --gemma3-config-json
      - {inputPath: gemma3_config_json}
      - --gemma3-model-py
      - {inputPath: gemma3_model_py}
      - --best-pt
      - {inputPath: best_pt}
      - --dataset-name
      - {inputValue: dataset_name}
      - --lora-rank
      - {inputValue: lora_rank}
      - --lora-alpha
      - {inputValue: lora_alpha}
      - --lora-dropout
      - {inputValue: lora_dropout}
      - --num-train-epochs
      - {inputValue: num_train_epochs}
      - --learning-rate
      - {inputValue: learning_rate}
      - --batch-size
      - {inputValue: batch_size}
      - --gradient-accumulation-steps
      - {inputValue: gradient_accumulation_steps}
      - --sliding-window
      - {inputValue: sliding_window}
      - --trained-model
      - {outputPath: trained_model}
      - --training-report
      - {outputPath: training_report}
