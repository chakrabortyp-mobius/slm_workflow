name: probir_MinIO
description: Packages model_py + config + best_weights + tokenizer into an MLServer-ready folder (with runtime.py + model-settings.json), uploads to MinIO, and outputs the s3:// Model URI.

inputs:
  # Artifacts (required)
  - {name: learned_weights, type: Model, description: "Trained weights (.pt/.pth) to serve"}
  - {name: model_py,        type: Data,  description: "Python file defining Gemma3Model"}
  - {name: model_config,    type: Data,  description: "JSON config used by the model"}
  - {name: tokenizer_json,  type: Model, description: "Tokenizer JSON used at training (required for raw-text inference)"}

  # Packaging / destination
  - {name: bucket,       type: String, default: "", description: "Target MinIO bucket"}
  - {name: app_name,     type: String, default: "", description: "Application grouping (folder prefix)"}
  - {name: model_name,   type: String, default: "", description: "Model name (also used in model-settings.json:name)"}
  - {name: version,      type: String, default: "", description: "Version/run-id (empty -> auto UTC timestamp)"}

  # MinIO access
  - {name: s3_endpoint,  type: String, default: "http://minio-service.kubeflow.svc.cluster.local:9000", description: "MinIO S3 endpoint (http/https)"}

outputs:
  - {name: model_uri,     type: String, description: "s3://bucket/app/model/version/ (directory URI)"}
  - {name: manifest_json, type: Data,   description: "Uploaded file manifest (sizes + sha256)"}

implementation:
  container:
    image: minio/mc:latest
    command:
      - sh
      - -eu
      - -c
      - |
        # Positional args from "args:" (below)
        WEIGHTS="$1"; MODEL_PY="$2"; CONFIG_JSON="$3"; TOKENIZER="$4"
        BUCKET="$5"; APP="$6"; NAME="$7"; VERSION="$8"; ENDPOINT="$9"
        OUT_URI="${10}"; OUT_MANIFEST="${11}"

        STAGE="/tmp/mlserver_pkg"
        mkdir -p "${STAGE}"

        # --- Validate requireds ---
        [ -n "${NAME}" ] || { echo "model_name is required" >&2; exit 1; }
        [ -n "${BUCKET}" ] || { echo "bucket is required" >&2; exit 1; }
        [ -n "${APP}" ] || { echo "app_name is required" >&2; exit 1; }
        [ -s "${MODEL_PY}" ] || { echo "model_py missing/empty" >&2; exit 1; }
        [ -s "${CONFIG_JSON}" ] || { echo "model_config missing/empty" >&2; exit 1; }
        [ -s "${TOKENIZER}" ] || { echo "tokenizer_json missing/empty (required for raw-text inference)" >&2; exit 1; }
        [ -s "${WEIGHTS}" ] || { echo "learned_weights missing/empty" >&2; exit 1; }

        # Auto version if empty
        if [ -z "${VERSION}" ]; then VERSION="$(date -u +'%Y%m%dT%H%M%SZ')"; fi

        # --- Stage artifacts ---
        cp "${MODEL_PY}"     "${STAGE}/model.py"
        cp "${CONFIG_JSON}"  "${STAGE}/config.json"
        cp "${TOKENIZER}"    "${STAGE}/tokenizer.json"
        # Support .pth/.pt from training brick
        if cp "${WEIGHTS}" "${STAGE}/best_weights.pth" 2>/dev/null; then :; else cp "${WEIGHTS}" "${STAGE}/best_weights.pt"; fi

        # --- Generate MLServer runtime shim (raw-text aware) ---
        cat > "${STAGE}/runtime.py" <<'PY'
from mlserver import MLModel
from mlserver.types import InferenceRequest, InferenceResponse, ResponseOutput, Parameters, TensorDatatype
import importlib.util, json, os, torch

def _load_module(path, mod_name="model"):
  spec = importlib.util.spec_from_file_location(mod_name, path)
  if spec is None or spec.loader is None:
    raise ImportError(f"Cannot import module from {path}")
  m = importlib.util.module_from_spec(spec); spec.loader.exec_module(m); return m

class Gemma3Runtime(MLModel):
  async def load(self) -> bool:
    base = os.getenv("MLSERVER_MODEL_PARAMETERS_URI", ".")
    with open(os.path.join(base, "config.json"), "r", encoding="utf-8") as f:
      self.cfg = json.load(f)
    self.cfg["dtype"] = torch.float16 if str(self.cfg.get("dtype")).lower()=="float16" else torch.float32

    ModelCls = getattr(_load_module(os.path.join(base, "model.py")), "Gemma3Model")
    self.model = ModelCls(self.cfg)
    w_pt  = os.path.join(base, "best_weights.pt")
    w_pth = os.path.join(base, "best_weights.pth")
    state = torch.load(w_pth if os.path.exists(w_pth) else w_pt, map_location="cpu")
    self.model.load_state_dict(state, strict=True)
    self.model.eval()

    from tokenizers import Tokenizer
    tok_path = os.path.join(base, "tokenizer.json")
    if not os.path.exists(tok_path):
      raise FileNotFoundError("tokenizer.json is required for raw-text inference")
    self.tokenizer = Tokenizer.from_file(tok_path)

    self.device = "cuda" if torch.cuda.is_available() else "cpu"
    self.model.to(self.device)
    self.ctx_len = int(self.cfg.get("context_length", 2048))
    return True

  def _encode_batch(self, texts):
    ids_list = [self.tokenizer.encode(str(t)).ids for t in texts]
    # truncate to context_length (keep tail)
    ids_list = [ids[-self.ctx_len:] if len(ids) > self.ctx_len else ids for ids in ids_list]
    # pad to max len
    maxlen = max(len(x) for x in ids_list)
    pad_id = self.tokenizer.token_to_id("[PAD]") or 0
    padded = [x + [pad_id]*(maxlen-len(x)) for x in ids_list]
    return torch.tensor(padded, dtype=torch.long, device=self.device)

  def _decode_id(self, tid):
    try: return self.tokenizer.decode([int(tid)])
    except Exception: return ""

  async def predict(self, request: InferenceRequest) -> InferenceResponse:
    # 1) Prefer raw text via V2 input "text"/"texts" else parameters.extra
    texts = None
    if request and request.inputs:
      for inp in request.inputs:
        if inp.name in ("text","texts"):
          val = inp.data
          if isinstance(val, list):
            texts = [v.decode("utf-8") if isinstance(v, (bytes,bytearray)) else str(v) for v in val]
          elif isinstance(val, (bytes, bytearray, str)):
            texts = [val.decode("utf-8") if isinstance(val,(bytes,bytearray)) else str(val)]
          break
    if texts is None and getattr(request, "parameters", None) and isinstance(request.parameters.extra, dict):
      extra = request.parameters.extra
      if "texts" in extra: texts = [str(x) for x in extra["texts"]]
      elif "text" in extra: texts = [str(extra["text"])]

    if texts is not None:
      input_ids = self._encode_batch(texts)
    else:
      # 2) Fallback: pre-tokenized IDs
      ids = None
      if request and request.inputs:
        for inp in request.inputs:
          if inp.name in ("input_ids","inputs","input_0"): ids = inp.data; break
      if ids is None:
        raise ValueError("Provide raw `text`/`texts` or pre-tokenized `input_ids`")
      if isinstance(ids, list) and ids and isinstance(ids[0], int):
        ids = [ids]
      input_ids = torch.tensor(ids, dtype=torch.long, device=self.device)

    with torch.inference_mode():
      logits, _ = self.model(input_ids)
      last = logits[:, -1, :]  # [B, vocab]
      next_ids = last.argmax(dim=-1).tolist()
      next_tokens = [self._decode_id(t) for t in next_ids]
      last_list = last.float().cpu().tolist()

    return InferenceResponse(
      model_name=self.name,
      outputs=[
        ResponseOutput(
          name="last_logits",
          shape=[len(last_list), len(last_list[0])],
          datatype=TensorDatatype.FP32,
          data=last_list,
          parameters=Parameters()
        ),
        ResponseOutput(
          name="next_token_id",
          shape=[len(next_ids)],
          datatype=TensorDatatype.INT64,
          data=next_ids,
          parameters=Parameters()
        ),
        ResponseOutput(
          name="next_token",
          shape=[len(next_tokens)],
          datatype=TensorDatatype.BYTES,
          data=next_tokens,
          parameters=Parameters()
        )
      ]
    )
PY

        # --- model-settings.json (MLServer descriptor) ---
        cat > "${STAGE}/model-settings.json" <<JSON
{
  "name": "${NAME}",
  "implementation": "runtime.Gemma3Runtime",
  "parameters": { "uri": "." }
}
JSON

        # --- Manifest (sizes + sha256 if available) ---
        {
          echo '{ "files": ['
          first=1
          for f in model.py runtime.py model-settings.json config.json best_weights.pth best_weights.pt tokenizer.json; do
            [ -f "${STAGE}/$f" ] || continue
            # size
            if stat -c%s / >/dev/null 2>&1; then
              sz=$(stat -c%s "${STAGE}/$f")
            else
              sz=$(stat -f%z "${STAGE}/$f" 2>/dev/null || echo 0)
            fi
            # sha256 (optional)
            if command -v sha256sum >/dev/null 2>&1; then
              sum=$(sha256sum "${STAGE}/$f" | awk '{print $1}')
            elif command -v shasum >/dev/null 2>&1; then
              sum=$(shasum -a 256 "${STAGE}/$f" | awk '{print $1}')
            else
              sum=""
            fi
            [ $first -eq 0 ] && echo ','
            printf '  { "name":"%s", "size":%s, "sha256":"%s" }' "$f" "$sz" "$sum"
            first=0
          done
          echo '] }'
        } > "${OUT_MANIFEST}"

        # --- Upload to MinIO ---
        mc alias set myminio "${ENDPOINT}" "${MINIO_ACCESS_KEY}" "${MINIO_SECRET_KEY}"
        mc mb --ignore-existing "myminio/${BUCKET}"

        DEST="myminio/${BUCKET}/${APP}/${NAME}/${VERSION}/"
        mc cp --recursive "${STAGE}/" "${DEST}"

        # Output s3-style directory URI
        printf 's3://%s/%s/%s/%s/\n' "${BUCKET}" "${APP}" "${NAME}" "${VERSION}" > "${OUT_URI}"
    args:
      - {inputPath: learned_weights}
      - {inputPath: model_py}
      - {inputPath: model_config}
      - {inputPath: tokenizer_json}
      - {inputValue: bucket}
      - {inputValue: app_name}
      - {inputValue: model_name}
      - {inputValue: version}
      - {inputValue: s3_endpoint}
      - {outputPath: model_uri}
      - {outputPath: manifest_json}
    env:
      - name: MINIO_ACCESS_KEY
        valueFrom: {secretKeyRef: {name: minio-creds, key: accesskey}}
      - name: MINIO_SECRET_KEY
        valueFrom: {secretKeyRef: {name: minio-creds, key: secretkey}}
