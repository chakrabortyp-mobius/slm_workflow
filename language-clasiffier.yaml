name: language classifier
description: Fine-tunes RoBERTa with frozen base and custom FC layers for text classification from CSV
  - name: csv_path
    type: String
  - name: text_column
    type: String
  - name: label_column
    type: String
  - name: test_size
    type: Float
  - name: max_sequence_length
    type: Integer
  - name: hidden_layer_1
    type: Integer
  - name: hidden_layer_2
    type: Integer
  - name: dropout
    type: Float
  - name: batch_size
    type: Integer
  - name: learning_rate
    type: Float
  - name: num_epochs
    type: Integer
  - name: pretrained_model
    type: String
outputs:
  - name: trained_model
    type: Model
  - name: label_encoder
    type: Data
  - name: training_metrics 
    type: Data
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - python3
      - -u
      - -c
      - |
        import argparse
        import logging
        import json
        import pickle
        import os
        import pandas as pd
        from sklearn.model_selection import train_test_split
        from sklearn.preprocessing import LabelEncoder
        import torch
        import torch.nn as nn
        from torch.utils.data import DataLoader, Dataset
        from transformers import AutoTokenizer, AutoModel, AdamW
       
        parser = argparse.ArgumentParser()
        parser.add_argument('--csv_path', type=str, required=True)
        parser.add_argument('--text_column', type=str, required=True)
        parser.add_argument('--label_column', type=str, required=True)
        parser.add_argument('--test_size', type=float, required=True)
        parser.add_argument('--max_sequence_length', type=int, required=True)
        parser.add_argument('--hidden_layer_1', type=int, required=True)
        parser.add_argument('--hidden_layer_2', type=int, required=True)
        parser.add_argument('--dropout', type=float, required=True)
        parser.add_argument('--batch_size', type=int, required=True)
        parser.add_argument('--learning_rate', type=float, required=True)
        parser.add_argument('--num_epochs', type=int, required=True)
        parser.add_argument('--pretrained_model', type=str, required=True)
        parser.add_argument('--trained_model', type=str, required=True)
        parser.add_argument('--label_encoder', type=str, required=True)
        parser.add_argument('--training_metrics', type=str, required=True)
       
        args = parser.parse_args()
       
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger("roberta_classifier")
       
        # Custom Dataset Class
        class TextDataset(Dataset):
            def __init__(self, texts, labels, tokenizer, max_len):
                self.texts = texts
                self.labels = labels
                self.tokenizer = tokenizer
                self.max_len = max_len

            def __len__(self):
                return len(self.texts)

            def __getitem__(self, item):
                text = str(self.texts[item])
                label = self.labels[item]

                encoding = self.tokenizer.encode_plus(
                    text,
                    add_special_tokens=True,
                    max_length=self.max_len,
                    padding='max_length',
                    truncation=True,
                    return_attention_mask=True,
                    return_tensors='pt',
                )
               
                return {
                    'input_ids': encoding['input_ids'].flatten(),
                    'attention_mask': encoding['attention_mask'].flatten(),
                    'labels': torch.tensor(label, dtype=torch.long)
                }
       
        # Model Definition
        class RoBERTaClassifier(nn.Module):
            def __init__(self, pretrained_model, num_classes, hidden_layers, dropout):
                super(RoBERTaClassifier, self).__init__()
               
                self.roberta = AutoModel.from_pretrained(pretrained_model)
               
                # Freeze RoBERTa parameters
                for param in self.roberta.parameters():
                    param.requires_grad = False
               
                # Build custom classification head
                layers = []
                input_dim = 768  # RoBERTa hidden size
               
                for hidden_dim in hidden_layers:
                    layers.append(nn.Linear(input_dim, hidden_dim))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(dropout))
                    input_dim = hidden_dim
               
                layers.append(nn.Linear(input_dim, num_classes))
                self.classifier = nn.Sequential(*layers)

            def forward(self, input_ids, attention_mask):
                outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
                cls_output = outputs[0][:, 0, :]
                logits = self.classifier(cls_output)
                return logits
       
        # Training Functions
        def train_epoch(model, data_loader, optimizer, criterion, device):
            model.train()
            total_loss = 0
            total_correct = 0
            total_samples = 0
           
            for batch in data_loader:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)
               
                optimizer.zero_grad()
                outputs = model(input_ids, attention_mask)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
               
                total_loss += loss.item()
                _, preds = torch.max(outputs, dim=1)
                total_correct += torch.sum(preds == labels).item()
                total_samples += labels.size(0)
           
            return total_loss / len(data_loader), total_correct / total_samples
       
        def eval_model(model, data_loader, criterion, device):
            model.eval()
            total_loss = 0
            total_correct = 0
            total_samples = 0
           
            with torch.no_grad():
                for batch in data_loader:
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                    labels = batch['labels'].to(device)
                   
                    outputs = model(input_ids, attention_mask)
                    loss = criterion(outputs, labels)
                   
                    total_loss += loss.item()
                    _, preds = torch.max(outputs, dim=1)
                    total_correct += torch.sum(preds == labels).item()
                    total_samples += labels.size(0)
           
            return total_loss / len(data_loader), total_correct / total_samples
       
        # Main Training Pipeline
        
        logger.info("RoBERTa Fine-tuning with Custom Classification Head")
        
       
        # Load data
        logger.info(f"Loading data from {args.csv_path}...")
        df = pd.read_csv(args.csv_path)
       
        # Encode labels
        label_encoder = LabelEncoder()
        df['encoded_label'] = label_encoder.fit_transform(df[args.label_column])
        num_classes = len(label_encoder.classes_)
       
        logger.info(f"Number of samples: {len(df)}")
        logger.info(f"Number of classes: {num_classes}")
        logger.info(f"Class labels: {list(label_encoder.classes_)}")
       
        # Split data
        train_df, val_df = train_test_split(
            df,
            test_size=args.test_size,
            random_state=42
        )
       
        logger.info(f"Train samples: {len(train_df)}, Validation samples: {len(val_df)}")
       
        # Load tokenizer
        logger.info(f"Loading tokenizer: {args.pretrained_model}")
        tokenizer = AutoTokenizer.from_pretrained(args.pretrained_model)
       
        # Create datasets
        train_dataset = TextDataset(
            texts=train_df[args.text_column].values,
            labels=train_df['encoded_label'].values,
            tokenizer=tokenizer,
            max_len=args.max_sequence_length
        )
       
        val_dataset = TextDataset(
            texts=val_df[args.text_column].values,
            labels=val_df['encoded_label'].values,
            tokenizer=tokenizer,
            max_len=args.max_sequence_length
        )
       
        # Create data loaders
        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=args.batch_size)
       
        # Setup device
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        logger.info(f"Using device: {device}")
       
        # Initialize model
        hidden_layers = [args.hidden_layer_1, args.hidden_layer_2]
        logger.info(f"Architecture: RoBERTa -> {args.hidden_layer_1} -> {args.hidden_layer_2} -> {num_classes}")
       
        model = RoBERTaClassifier(
            pretrained_model=args.pretrained_model,
            num_classes=num_classes,
            hidden_layers=hidden_layers,
            dropout=args.dropout
        )
        model.to(device)
       
        # Setup optimizer and loss
        optimizer = AdamW(model.parameters(), lr=args.learning_rate)
        criterion = nn.CrossEntropyLoss()
       
        # Training loop
        logger.info(f"Starting training for {args.num_epochs} epochs...")
        
       
        metrics = {
            "epochs": [],
            "train_loss": [],
            "train_accuracy": [],
            "val_loss": [],
            "val_accuracy": []
        }
       
        for epoch in range(args.num_epochs):
            logger.info(f"Epoch {epoch + 1}/{args.num_epochs}")
           
            train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)
            val_loss, val_acc = eval_model(model, val_loader, criterion, device)
           
            metrics["epochs"].append(epoch + 1)
            metrics["train_loss"].append(float(train_loss))
            metrics["train_accuracy"].append(float(train_acc))
            metrics["val_loss"].append(float(val_loss))
            metrics["val_accuracy"].append(float(val_acc))
           
            logger.info(f"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}")
            logger.info(f"  Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}")
       
        # Create output directories
        os.makedirs(os.path.dirname(args.trained_model), exist_ok=True)
        os.makedirs(os.path.dirname(args.label_encoder), exist_ok=True)
        os.makedirs(os.path.dirname(args.training_metrics), exist_ok=True)
       
        # Save model
        
        logger.info(f"Saving model to {args.trained_model}")
        torch.save(model.state_dict(), args.trained_model)
       
        # Save label encoder
        logger.info(f"Saving label encoder to {args.label_encoder}")
        with open(args.label_encoder, 'wb') as f:
            pickle.dump(label_encoder, f)
       
        # Save metrics
        logger.info(f"Saving training metrics to {args.training_metrics}")
        with open(args.training_metrics, 'w') as f:
            json.dump(metrics, f, indent=2)
       
        
        logger.info("Training complete!")
        logger.info(f"Final validation accuracy: {val_acc:.4f}")
        
    args:
      - --csv_path
      - {inputValue: csv_path}
      - --text_column
      - {inputValue: text_column}
      - --label_column
      - {inputValue: label_column}
      - --test_size
      - {inputValue: test_size}
      - --max_sequence_length
      - {inputValue: max_sequence_length}
      - --hidden_layer_1
      - {inputValue: hidden_layer_1}
      - --hidden_layer_2
      - {inputValue: hidden_layer_2}
      - --dropout
      - {inputValue: dropout}
      - --batch_size
      - {inputValue: batch_size}
      - --learning_rate
      - {inputValue: learning_rate}
      - --num_epochs
      - {inputValue: num_epochs}
      - --pretrained_model
      - {inputValue: pretrained_model}
      - --trained_model
      - {outputPath: trained_model}
      - --label_encoder
      - {outputPath: label_encoder}
      - --training_metrics
      - {outputPath: training_metrics}
