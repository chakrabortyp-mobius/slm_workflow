name: GRPOdataset
description: Prepares clean tool-calling dataset for GRPO training (no chat template)

inputs:
  - {name: dataset_name, type: String, description: 'HuggingFace dataset name'}
  - {name: dataset_split, type: String, description: 'Dataset split name'}

outputs:
  - {name: grpo_dataset, type: Dataset, description: 'Processed dataset for GRPO training'}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v24-gpu
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import pickle
        from datasets import load_dataset
        from collections import OrderedDict
        import argparse

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--dataset_name', type=str, required=True)
            parser.add_argument('--dataset_split', type=str, required=True)
            parser.add_argument('--grpo_dataset', type=str, required=True)
            args = parser.parse_args()

            print("===================================================")
            print("GRPO Dataset Preparation (NO Chat Template)")
            print("Dataset:", args.dataset_name)
            print("Split:", args.dataset_split)
            print("===================================================")

            print("Loading dataset...")
            dataset = load_dataset(args.dataset_name, split=args.dataset_split)
            print(f"Loaded {len(dataset)} examples")

            # --- NORMALIZATION FUNCTION ---
            def convert_to_grpo_format(example):
                if "conversations" not in example:
                    return {"prompt": None, "answer": None}

                convos = example["conversations"]

                # Role normalization
                role_map = {
                    "human": "user",
                    "gpt": "assistant",
                    "model": "assistant",
                    "assistant": "assistant",
                    "system": "system",
                    "tool": "tool",
                }

                # Find first tool_call from model
                tool_call_index = None
                for i, conv in enumerate(convos):
                    role = conv.get("role") or conv.get("from", "")
                    content = conv.get("content") or conv.get("value", "")
                    if role in ["model"] and "<tool_call>" in content:
                        tool_call_index = i
                        break

                if tool_call_index is None:
                    return {"prompt": None, "answer": None}

                # Build prompt list
                prompt_messages = []
                for conv in convos[:tool_call_index]:
                    raw_role = conv.get("role") or conv.get("from", "")
                    mapped_role = role_map.get(raw_role, "user")
                    content = conv.get("content") or conv.get("value", "")

                    prompt_messages.append(
                        OrderedDict([
                            ("role", mapped_role),
                            ("content", content),
                        ])
                    )

                # Extract answer content
                answer_conv = convos[tool_call_index]
                answer_content = answer_conv.get("content") or answer_conv.get("value", "")

                if not prompt_messages or not answer_content:
                    return {"prompt": None, "answer": None}

                return {
                    "prompt": prompt_messages,
                    "answer": answer_content
                }

            # Apply transformation
            print("Processing dataset...")
            processed = dataset.map(convert_to_grpo_format, remove_columns=dataset.column_names)

            original_size = len(processed)
            processed = processed.filter(lambda x: x["prompt"] is not None and x["answer"] is not None)

            print(f"Filtered to {len(processed)} valid examples (from {original_size})")

            # Display samples
            if len(processed) > 0:
                print("\\n=======================================")
                print("SAMPLE PROCESSED EXAMPLE:")
                print("=======================================")
                print("PROMPT:")
                print(processed[0]["prompt"])
                print("\\nANSWER:")
                print(processed[0]["answer"])
                print("=======================================")

            # Save dataset as pickle
            out_dir = os.path.dirname(args.grpo_dataset)
            if out_dir:
                os.makedirs(out_dir, exist_ok=True)

            records = []
            for ex in processed:
                records.append({
                    "prompt": ex["prompt"],
                    "answer": ex["answer"],
                })
            
            with open(args.grpo_dataset, "wb") as f:
                pickle.dump(records, f)



            print("\\n==========================================")
            print("SUCCESS!")
            print(f"Saved GRPO dataset to: {args.grpo_dataset}")
            print(f"Total examples: {len(processed)}")
            print("==========================================")

        if __name__ == '__main__':
            main()

    args:
      - --dataset_name
      - {inputValue: dataset_name}
      - --dataset_split
      - {inputValue: dataset_split}
      - --grpo_dataset
      - {outputPath: grpo_dataset}
