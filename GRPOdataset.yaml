name: GRPOdataset
description: Prepares tool-calling dataset for GRPO training with correct prompt/answer extraction

inputs:
  - {name: dataset_name, type: String, description: 'HuggingFace dataset name'}
  - {name: dataset_split, type: String, description: 'Dataset split name'}
  - {name: max_samples, type: Integer, description: 'Max number of samples (0 = all)'}

outputs:
  - {name: grpo_dataset, type: Dataset, description: 'Processed dataset for GRPO training'}

implementation:
  container:
    image: gurpreetgandhi/nesy-factory:v24-gpu
    command:
      - sh
      - -c
      - |
        exec "$0" "$@"
      - python3
      - -u
      - -c
      - |
        import os
        import re
        import pickle
        from datasets import load_dataset
        import argparse

        # Regex to parse chat template blocks
        BLOCK_RE = re.compile(
            r"<\|im_start\|\>(.*?)\n(.*?)<\|im_end\|\>",
            re.DOTALL
        )

        def parse_chat_blocks(text):

            blocks = []
            for role, content in BLOCK_RE.findall(text):
                blocks.append({"role": role.strip(), "content": content.strip()})
            return blocks

        def convert(example):

            if "text" not in example:
                return None

            blocks = parse_chat_blocks(example["text"])

            system_block = None
            user_block = None
            assistant_block = None

            # Iterate through blocks in order
            for b in blocks:
                role = b["role"]
                content = b["content"]

                if role == "system" and system_block is None:
                    system_block = b

                elif role == "user" and user_block is None:
                    user_block = b

                elif role == "assistant" and assistant_block is None:
                    assistant_block = b
                    break

            # Must have system + user + assistant
            if system_block is None or user_block is None or assistant_block is None:
                return None

            # Skip if first assistant response does NOT contain tool_call
            if "<tool_call>" not in assistant_block["content"]:
                return None

            # Build prompt (everything up to assistant response)
            prompt = (
                "<|im_start|>system\n" + system_block["content"] + "<|im_end|>\n" +
                "<|im_start|>user\n" + user_block["content"] + "<|im_end|>"
            )

            # Build answer (the expected assistant response)
            answer = (
                "<|im_start|>assistant\n" + assistant_block["content"] + "<|im_end|>"
            )

            return {"prompt": prompt, "answer": answer}

        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument('--dataset_name', type=str, required=True)
            parser.add_argument('--dataset_split', type=str, required=True)
            parser.add_argument('--max_samples', type=int, required=True)
            parser.add_argument('--grpo_dataset', type=str, required=True)
            args = parser.parse_args()

            print("========================")
            print("Tool Calling GRPO Data Preparation")
            print("Dataset:", args.dataset_name)
            print("Split:", args.dataset_split)
            print("============================")

            # Load dataset from HuggingFace
            print("Loading dataset...")
            dataset = load_dataset(args.dataset_name, split=args.dataset_split)
            print("Loaded", len(dataset), "examples")

            # Limit to max_samples if specified
            if args.max_samples > 0:
                dataset = dataset.select(range(min(args.max_samples, len(dataset))))
                print("Using first", len(dataset), "samples")

            # Convert to GRPO format
            print("Processing dataset...")
            processed = dataset.map(
                convert,
                remove_columns=dataset.column_names
            )

            # Filter out failed conversions (None values)
            original_size = len(processed)
            processed = processed.filter(lambda x: x["prompt"] is not None)
            print("Filtered:", len(processed), "valid examples (from", original_size, ")")

            # Print sample
            if len(processed) > 0:
                print("\n" + "=" * 60)
                print("SAMPLE PROCESSED EXAMPLE:")
                print("-" * 60)
                print("PROMPT:")
                print(processed[0]["prompt"][:300], "...")
                print("\nANSWER:")
                print(processed[0]["answer"][:300], "...")
                print("=" * 60)

            # Save as pickle for GRPO trainer
            out_dir = os.path.dirname(args.grpo_dataset)
            if out_dir:
                os.makedirs(out_dir, exist_ok=True)

            with open(args.grpo_dataset, "wb") as f:
                pickle.dump(processed, f)

            print("=========================")
            print("SUCCESS!")
            print("Saved GRPO dataset to:", args.grpo_dataset)
            print("Total processed examples:", len(processed))
            print("=========================")

        if __name__ == '__main__':
            main()

    args:
      - --dataset_name
      - {inputValue: dataset_name}
      - --dataset_split
      - {inputValue: dataset_split}
      - --max_samples
      - {inputValue: max_samples}
      - --grpo_dataset
      - {outputPath: grpo_dataset}
