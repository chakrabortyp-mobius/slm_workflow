name: probir_Train model_1644
description: Tokenizes corpus to memmaps and trains Gemma3 with AMP, warmup→cosine LR, gradient accumulation, and best/final checkpoints. 

inputs:
  - name: tokenizer_json
    type: Model
  - name: train_corpus
    type: Data
  - name: model_config
    type: Data
  - name: model_weights
    type: Model
  - name: model_py_in
    type: Data

  # Hyperparameters (same defaults)
  - name: learning_rate
    type: String
    default: "1e-4"
  - name: min_lr
    type: String
    default: "1e-5"
  - name: warmup_steps
    type: Integer
    default: "1000"
  - name: max_iters
    type: Integer
    default: "150000"
  - name: batch_size
    type: Integer
    default: "32"
  - name: block_size
    type: Integer
    default: "128"
  - name: grad_accum
    type: Integer
    default: "32"
  - name: eval_interval
    type: Integer
    default: "1000"
  - name: eval_iters
    type: Integer
    default: "500"
  - name: weight_decay
    type: String
    default: "0.1"
  - name: beta2
    type: String
    default: "0.95"
  - name: clip_grad_norm
    type: String
    default: "0.5"
  - name: val_fraction
    type: String
    default: "0.1"
  - name: num_proc
    type: Integer
    default: "8"

outputs:
  - name: best_weights
    type: Model
  - name: final_weights
    type: Model
  - name: training_report
    type: Data
  - name: loss_curve_csv
    type: Data
  - name: model_py
    type: Data
  - name: schema_json
    type: String   # String so downstream receives the JSON text

implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - python3
      - -u
      - -c
      - |-
        import subprocess, sys
        import os, json, math, argparse, numpy as np, shutil, importlib.util, torch
        import torch.nn.functional as F
        from contextlib import nullcontext
        from tokenizers import Tokenizer
        from datasets import load_dataset
        from tqdm import tqdm


        def build_train_schema(args, meta, best_val_loss=None, total_params=None, total_updates=None, warmup_updates=None):
            return {
                # Inputs (as provided/resolved)
                "tokenizer_json": args.tokenizer_json,
                "train_corpus": args.train_corpus,
                "model_config": args.model_config,
                "model_weights": args.model_weights,
                "model_py_in": args.model_py_in,

                # Effective hyperparameters
                "optimizer": "AdamW",
                "scheduler": "WarmupLinear → CosineAnnealingLR",
                "learning_rate": args.learning_rate,
                "min_lr": args.min_lr,
                "warmup_steps": args.warmup_steps,
                "max_iters": args.max_iters,
                "batch_size": args.batch_size,
                "block_size": args.block_size,
                "grad_accum": args.grad_accum,
                "eval_interval": args.eval_interval,
                "eval_iters": args.eval_iters,
                "weight_decay": args.weight_decay,
                "beta1": 0.9,
                "beta2": args.beta2,
                "clip_grad_norm": args.clip_grad_norm,
                "val_fraction": args.val_fraction,
                "num_proc": args.num_proc,

                # Run metadata
                "vocab_size": meta.get("vocab_size"),
                "train_tokens": meta.get("train_tokens"),
                "val_tokens": meta.get("val_tokens"),
                "bin_dtype": meta.get("bin_dtype"),
                "best_val_loss": best_val_loss,
                "total_updates": total_updates,
                "warmup_updates": warmup_updates,
                "total_learnable_parameters": total_params
            }

        def choose_bin_dtype(vocab_size):
            if vocab_size <= (1 << 16):  return np.uint16
            if vocab_size <= (1 << 32):  return np.uint32
            return np.uint64

        def tokenize_to_bins(tokenizer_path, train_txt, val_fraction, num_proc):
            tok = Tokenizer.from_file(tokenizer_path)
            vocab_size = tok.get_vocab_size()
            BIN_DTYPE = choose_bin_dtype(vocab_size)

            ds_single = load_dataset("text", data_files={"train": train_txt})
            split = ds_single["train"].train_test_split(test_size=float(val_fraction), seed=42)
            ds = {"train": split["train"], "val": split["test"]}

            def proc(ex):
                ids = tok.encode(ex["text"]).ids
                return {"ids": ids, "len": len(ids)}

            nproc = max(1, min(int(num_proc), (os.cpu_count() or 1)))
            tokenized = {}
            for sp, dset in ds.items():
                tokenized[sp] = dset.map(proc, remove_columns=dset.column_names, desc=f"tokenizing {sp}", num_proc=nproc)

            def write_split(dset, filename):
                total = int(np.sum(dset["len"], dtype=np.uint64))
                if total == 0: raise ValueError(f"{filename}: no tokens to write")
                arr = np.memmap(filename, dtype=BIN_DTYPE, mode="w+", shape=(total,))
                shard_size = max(1, len(dset) // 1024)
                total_shards = max(1, (len(dset) + shard_size - 1) // shard_size)
                idx = 0
                for i in tqdm(range(total_shards), desc=f"writing {filename}"):
                    start, stop = i * shard_size, min(len(dset), (i + 1) * shard_size)
                    if start >= stop: continue
                    batch = dset.select(range(start, stop)).with_format(type="numpy")
                    ids_list = batch["ids"]
                    if not ids_list: continue
                    arr_batch = np.concatenate([np.asarray(x, dtype=BIN_DTYPE) for x in ids_list])
                    arr[idx: idx + len(arr_batch)] = arr_batch
                    idx += len(arr_batch)
                arr.flush()
                return total

            train_tokens = write_split(tokenized["train"], "train.bin")
            val_tokens   = write_split(tokenized["val"],   "val.bin")

            meta = {"vocab_size": vocab_size, "train_tokens": int(train_tokens), "val_tokens": int(val_tokens),
                    "bin_dtype": str(BIN_DTYPE.__name__)}
            with open("bin_meta.json", "w", encoding="utf-8") as f: json.dump(meta, f, indent=2)
            return meta


        def make_get_batch(device, device_type, block_size, batch_size):
            with open("bin_meta.json", "r", encoding="utf-8") as f:
                meta = json.load(f)
            dtype_map = {"uint16": np.uint16, "uint32": np.uint32, "uint64": np.uint64}
            BIN_DTYPE = dtype_map[meta["bin_dtype"]]
            TRAIN_BIN, VAL_BIN = "train.bin", "val.bin"

            def get_batch(split):
                path = TRAIN_BIN if split == "train" else VAL_BIN
                data = np.memmap(path, dtype=BIN_DTYPE, mode="r")
                if len(data) <= block_size:
                    raise ValueError(f"{path}: not enough tokens for block_size={block_size}")
                ix = torch.randint(len(data) - block_size - 1, (batch_size,))
                x = torch.stack([torch.from_numpy(np.asarray(data[i:i+block_size], dtype=np.int64)) for i in ix])
                y = torch.stack([torch.from_numpy(np.asarray(data[i+1:i+block_size+1], dtype=np.int64)) for i in ix])
                if device_type == "cuda":
                    x = x.pin_memory().to(device, non_blocking=True); y = y.pin_memory().to(device, non_blocking=True)
                else:
                    x, y = x.to(device), y.to(device)
                return x, y
            return get_batch


        def estimate_loss(model, get_batch, eval_iters, ctx):
            out = {}
            model.eval()
            with torch.inference_mode():
                for split in ["train", "val"]:
                    losses = torch.zeros(eval_iters, device="cpu")
                    for k in range(eval_iters):
                        X, Y = get_batch(split)
                        with ctx:
                            _, loss = model(X, Y)
                        losses[k] = loss.item()
                    out[split] = losses.mean().item()
            model.train()
            return out


        def find_model_py_file(directory):
            print(f"[DEBUG] Searching for .py file in: {directory}")
            if os.path.isfile(directory):
                print(f"[DEBUG] Path is already a file: {directory}")
                return directory
            if not os.path.isdir(directory):
                raise ValueError(f"Path {directory} is neither a file nor a directory")
            print(f"[DEBUG] Directory contents: {os.listdir(directory)}")
            py_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(".py")]
            if not py_files:
                raise FileNotFoundError(f"No .py file found in directory {directory}")
            if len(py_files) > 1:
                print(f"[WARN] Multiple .py files found in {directory}, using first one: {py_files[0]}")
            print(f"[DEBUG] Selected .py file: {py_files[0]}")
            return py_files[0]


        def load_model_class_from_file(py_path, class_name="Gemma3Model"):
            if not py_path.endswith('.py'):
                print(f"[DEBUG] File {py_path} doesn't have .py extension, creating temporary .py file")
                temp_py_path = py_path + '.py'
                import shutil as _sh
                _sh.copyfile(py_path, temp_py_path)
                py_path = temp_py_path
                print(f"[DEBUG] Created temporary file: {py_path}")
            spec = importlib.util.spec_from_file_location("gemma3_model", py_path)
            if spec is None or spec.loader is None:
                raise ImportError(f"Could not load spec from {py_path}")
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)
            if not hasattr(mod, class_name):
                raise AttributeError(f"{py_path} does not define {class_name}")
            return getattr(mod, class_name)

        def main():
            ap = argparse.ArgumentParser()
            ap.add_argument("--tokenizer-json", required=True)
            ap.add_argument("--train-corpus", required=True)
            ap.add_argument("--model-config", required=True)
            ap.add_argument("--model-weights", required=True)
            ap.add_argument("--model-py-in", required=True)
            ap.add_argument("--model-py-out", required=True)

            ap.add_argument("--learning-rate", type=float, required=True)
            ap.add_argument("--min-lr", type=float, required=True)
            ap.add_argument("--warmup-steps", type=int, required=True)
            ap.add_argument("--max-iters", type=int, required=True)
            ap.add_argument("--batch-size", type=int, required=True)
            ap.add_argument("--block-size", type=int, required=True)
            ap.add_argument("--grad-accum", type=int, required=True)
            ap.add_argument("--eval-interval", type=int, required=True)
            ap.add_argument("--eval-iters", type=int, required=True)
            ap.add_argument("--weight-decay", type=float, required=True)
            ap.add_argument("--beta2", type=float, required=True)
            ap.add_argument("--clip-grad-norm", type=float, required=True)
            ap.add_argument("--val-fraction", type=float, required=True)
            ap.add_argument("--num-proc", type=int, required=True)

            ap.add_argument("--best-weights", required=True)
            ap.add_argument("--final-weights", required=True)
            ap.add_argument("--training-report", required=True)
            ap.add_argument("--loss-curve-csv", required=True)
            ap.add_argument("--schema-output", required=True)
            args = ap.parse_args()


            model_py_path = find_model_py_file(args.model_py_in)
            os.makedirs(os.path.dirname(args.model_py_out) or ".", exist_ok=True)
            shutil.copyfile(model_py_path, args.model_py_out)

            device = "cuda" if torch.cuda.is_available() else "cpu"
            device_type = "cuda" if device == "cuda" else "cpu"
            print(f"[INFO] Training on {device.upper()}")


            with open(args.model_config, "r", encoding="utf-8") as f:
                cfg = json.load(f)
            cfg["dtype"] = torch.float16 if str(cfg.get("dtype")) == "float16" else torch.float32


            if not (args.min_lr <= args.learning_rate):
                raise ValueError(f"min_lr ({args.min_lr}) must be <= learning_rate ({args.learning_rate})")
            if args.block_size > int(cfg["context_length"]):
                raise ValueError(f"block_size ({args.block_size}) must be <= context_length ({cfg['context_length']})")
            if "sliding_attention" in cfg.get("layer_types", []):
                if args.block_size > int(cfg["sliding_window"]):
                    raise ValueError(f"block_size ({args.block_size}) must be <= sliding_window ({cfg['sliding_window']}) for sliding layers")


            tok = Tokenizer.from_file(args.tokenizer_json)
            if tok.get_vocab_size() != int(cfg["vocab_size"]):
                print(f"[WARN] tokenizer vocab {tok.get_vocab_size()} != model config vocab {cfg['vocab_size']}")


            Gemma3Model = load_model_class_from_file(model_py_path, "Gemma3Model")
            model = Gemma3Model(cfg).to(device)
            state = torch.load(args.model_weights, map_location=device)
            model.load_state_dict(state, strict=True)
            model.train()


            total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)


            ptdtype = torch.bfloat16 if (device_type == "cuda" and torch.cuda.is_bf16_supported()) else torch.float16
            ctx = nullcontext() if device_type == "cpu" else torch.amp.autocast(device_type=device_type, dtype=ptdtype)
            scaler = torch.cuda.amp.GradScaler(enabled=(device_type == "cuda" and ptdtype == torch.float16))


            meta = tokenize_to_bins(args.tokenizer_json, args.train_corpus, args.val_fraction, args.num_proc)
            get_batch = make_get_batch(device, device_type, args.block_size, args.batch_size)


            optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, betas=(0.9, args.beta2),
                                          weight_decay=args.weight_decay, eps=1e-9)
            from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR
            total_updates = math.ceil(args.max_iters / max(1, args.grad_accum))
            warm_updates  = min(args.warmup_steps, max(1, total_updates))
            sched_warm = LinearLR(optimizer, total_iters=warm_updates)
            sched_cos  = CosineAnnealingLR(optimizer, T_max=max(1, total_updates - warm_updates), eta_min=args.min_lr)
            scheduler  = SequentialLR(optimizer, schedulers=[sched_warm, sched_cos], milestones=[warm_updates])

            best_val = float("inf")
            train_curve, val_curve, lr_curve = [], [], []
            updates = 0

            pbar = tqdm(range(args.max_iters), desc="train micro-steps")
            for step in pbar:
                X, Y = get_batch("train")
                with ctx:
                    _, loss = model(X, Y)
                    (loss / args.grad_accum).backward()

                if ((step + 1) % args.grad_accum == 0) or (step + 1 == args.max_iters):
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=args.clip_grad_norm)
                    if scaler.is_enabled():
                        scaler.step(optimizer); scaler.update()
                    else:
                        optimizer.step()
                    optimizer.zero_grad(set_to_none=True)
                    scheduler.step()
                    updates += 1

                    if updates % max(1, args.eval_interval) == 0:
                        losses = estimate_loss(model, get_batch, args.eval_iters, ctx)
                        train_curve.append(losses["train"])
                        val_curve.append(losses["val"])
                        lr_curve.append(optimizer.param_groups[0]["lr"])
                        pbar.set_postfix(train=f"{losses['train']:.4f}", val=f"{losses['val']:.4f}", lr=f"{optimizer.param_groups[0]['lr']:.2e}")
                        if losses["val"] < best_val:
                            best_val = losses["val"]
                            os.makedirs(os.path.dirname(args.best_weights) or ".", exist_ok=True)
                            torch.save(model.state_dict(), args.best_weights)


            os.makedirs(os.path.dirname(args.final_weights) or ".", exist_ok=True)
            torch.save(model.state_dict(), args.final_weights)


            os.makedirs(os.path.dirname(args.training_report) or ".", exist_ok=True)
            report = {
                "best_val_loss": best_val,
                "final_lr": optimizer.param_groups[0]["lr"],
                "updates": updates,
                "max_iters": args.max_iters,
                "batch_size": args.batch_size,
                "block_size": args.block_size,
                "grad_accum": args.grad_accum,
                "train_tokens": meta.get("train_tokens"),
                "val_tokens": meta.get("val_tokens"),
                "total_updates": total_updates,
                "warmup_updates": warm_updates
            }
            with open(args.training_report, "w", encoding="utf-8") as f:
                json.dump(report, f, indent=2)

            # Loss curve CSV
            import csv
            os.makedirs(os.path.dirname(args.loss_curve_csv) or ".", exist_ok=True)
            with open(args.loss_curve_csv, "w", newline="", encoding="utf-8") as f:
                w = csv.writer(f); w.writerow(["update","train_loss","val_loss","lr"])
                for i,(tr,va,lr) in enumerate(zip(train_curve, val_curve, lr_curve), start=1):
                    w.writerow([i, tr, va, lr])


            schema_list = [
                {"epoch": int(i), "loss": float(tr), "validation_loss": float(va)}
                for i, (tr, va) in enumerate(zip(train_curve, val_curve), start=1)
            ]

            # Write compact JSON so it passes as a String param downstream
            os.makedirs(os.path.dirname(args.schema_output) or ".", exist_ok=True)
            with open(args.schema_output, "w", encoding="utf-8") as f:
                json.dump(schema_list, f, separators=(",", ":"))

            print("[DONE] best:", best_val, "final saved:", args.final_weights)

        if __name__ == "__main__":
            main()
    args:
      - --tokenizer-json
      - {inputPath: tokenizer_json}
      - --train-corpus
      - {inputPath: train_corpus}
      - --model-config
      - {inputPath: model_config}
      - --model-weights
      - {inputPath: model_weights}
      - --model-py-in
      - {inputPath: model_py_in}

      - --learning-rate
      - {inputValue: learning_rate}
      - --min-lr
      - {inputValue: min_lr}
      - --warmup-steps
      - {inputValue: warmup_steps}
      - --max-iters
      - {inputValue: max_iters}
      - --batch-size
      - {inputValue: batch_size}
      - --block-size
      - {inputValue: block_size}
      - --grad-accum
      - {inputValue: grad_accum}
      - --eval-interval
      - {inputValue: eval_interval}
      - --eval-iters
      - {inputValue: eval_iters}
      - --weight-decay
      - {inputValue: weight_decay}
      - --beta2
      - {inputValue: beta2}
      - --clip-grad-norm
      - {inputValue: clip_grad_norm}
      - --val-fraction
      - {inputValue: val_fraction}
      - --num-proc
      - {inputValue: num_proc}

      - --best-weights
      - {outputPath: best_weights}
      - --final-weights
      - {outputPath: final_weights}
      - --training-report
      - {outputPath: training_report}
      - --loss-curve-csv
      - {outputPath: loss_curve_csv}
      - --model-py-out
      - {outputPath: model_py}
      - --schema-output
      - {outputPath: schema_json}
