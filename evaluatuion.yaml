name: Evaluate Gemma3 Model
description: Evaluates a trained Gemma3 model using BLEU and ROUGE metrics with temperature and top_k sampling
inputs:
  - name: model_py
    type: Data
    description: "Python file OR directory containing Gemma3 model class"
  - name: model_config
    type: Data
    description: "JSON config file for the model"
  - name: model_weights
    type: Model
    description: "Trained weights (.pth file)"
  - name: tokenizer_json
    type: Model
    description: "Tokenizer JSON file"
  - name: eval_data
    type: Data
    description: "Evaluation data: plain text file for chunking"
  - name: experiment_name
    type: String
    default: "gemma3_eval"
    description: "MLflow experiment name (optional)"
  - name: max_new_tokens
    type: Integer
    default: "64"
    description: Maximum number of tokens to generate
  - name: temperature
    type: Float
    default: "1.0"
    description: Sampling temperature (higher = more random, lower = more deterministic)
  - name: top_k
    type: Integer
    default: "50"
    description: Number of top tokens to sample from (top-k sampling)
  - name: random_offset
    type: Integer
    default: "0"
    description: Random offset for chunk extraction
outputs:
  - name: metrics_json
    type: Metrics
    description: "KFP Metrics JSON (UI-compatible)"
  - name: results_csv
    type: Data
    description: "CSV with prompts, ground truth, and predictions"
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - python3
      - -u
      - -c
      - |-
        import subprocess, sys
        pkgs = ["mlflow", "rouge-score", "pandas", "sacrebleu"]
        for p in pkgs:
            try:
                __import__(p.replace("-", "_"))
                print(f"[INFO] Python package already present: {p}")
            except Exception:
                print(f"[INFO] Installing Python package: {p}")
                subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-cache-dir", p])
        
        import argparse, json, os, importlib.util, random
        import shutil
        import torch
        import torch.nn.functional as F
        import pandas as pd
        import mlflow
        from tokenizers import Tokenizer
        from rouge_score import rouge_scorer
        from sacrebleu.metrics import BLEU

        # ----------------- helpers -----------------
        def find_model_py_file(directory):
            print(f"[DEBUG] Searching for .py file in: {directory}")
            if os.path.isfile(directory):
                print(f"[DEBUG] Path is already a file: {directory}")
                return directory
            if not os.path.isdir(directory):
                raise ValueError(f"Path {directory} is neither a file nor a directory")
            print(f"[DEBUG] Directory contents: {os.listdir(directory)}")
            py_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(".py")]
            if not py_files:
                raise FileNotFoundError(f"No .py file found in directory {directory}")
            if len(py_files) > 1:
                print(f"[WARN] Multiple .py files found, using first: {py_files[0]}")
            print(f"[DEBUG] Selected .py file: {py_files[0]}")
            return py_files[0]

        def load_module_from_path(py_path, mod_name="gemma3_model"):
            if not py_path.endswith('.py'):
                print(f"[DEBUG] File {py_path} doesn't have .py extension, creating temporary .py file")
                temp_py_path = py_path + '.py'
                shutil.copyfile(py_path, temp_py_path)
                py_path = temp_py_path
                print(f"[DEBUG] Created temporary file: {py_path}")
            if not os.path.exists(py_path):
                raise FileNotFoundError(f"Model file not found: {py_path}")
            spec = importlib.util.spec_from_file_location(mod_name, py_path)
            if spec is None or spec.loader is None:
                raise ImportError(f"Cannot load module from: {py_path}")
            module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(module)
            return module

        def generate_text(model, tokenizer, prompt, bos_id, eos_id, context_len, max_new_tokens=64, temperature=1.0, top_k=50):
            device = next(model.parameters()).device
            model.eval()
            
            ids = tokenizer.encode(prompt).ids
            if bos_id is not None:
                ids = [bos_id] + ids
            x = torch.tensor([ids], dtype=torch.long, device=device)

            for _ in range(max_new_tokens):
                with torch.no_grad():
                    model_output = model(x)
                    if isinstance(model_output, tuple):
                        logits = model_output[0][0, -1]
                    else:
                        logits = model_output[0, -1]

                logits = logits / temperature

                if top_k is not None:
                    indices_to_remove = logits < torch.topk(logits, top_k).values[-1]
                    logits[indices_to_remove] = -float("Inf")

                probs = F.softmax(logits, dim=-1)
                next_id = torch.multinomial(probs, 1).item()
                ids.append(next_id)

                if eos_id is not None and next_id == eos_id:
                    break

                if len(ids) > context_len:
                    ids = ids[-context_len:]
                x = torch.tensor([ids], dtype=torch.long, device=device)

            dec_ids = ids[1:] if (bos_id is not None and len(ids) and ids[0] == bos_id) else ids
            return tokenizer.decode(dec_ids)

        def prepare_eval_data(eval_data_path, tokenizer, random_offset):
            # Read the full text
            with open(eval_data_path, "r", encoding="utf-8") as f:
                full_text = f.read()
            
            print(f"[INFO] Full text length: {len(full_text)} characters")
            
            # Tokenize the full text
            tokens = tokenizer.encode(full_text).ids
            print(f"[INFO] Total tokens: {len(tokens)}")
            
            # Create chunks with random offset
            chunk_size = 128
            max_offset = min(random_offset, len(tokens) - 128)
            
            chunks = [tokens[i + max_offset:i + max_offset + chunk_size] 
                     for i in range(0, len(tokens) - 128, chunk_size)]
            
            print(f"[INFO] Created {len(chunks)} chunks with offset {max_offset}")
            
            # Split each chunk into prompt and ground truth (64 tokens each)
            prompt_ground_truth_pairs = []
            for chunk in chunks:
                if len(chunk) >= 128:
                    prompt = chunk[:64]
                    ground_truth = chunk[64:128]
                    prompt_text = ' '.join(tokenizer.decode(prompt).split())
                    ground_truth_text = ' '.join(tokenizer.decode(ground_truth).split())
                    prompt_ground_truth_pairs.append((prompt_text, ground_truth_text))
            
            print(f"[INFO] Created {len(prompt_ground_truth_pairs)} prompt-ground_truth pairs")
            return prompt_ground_truth_pairs

        # ----------------- main -----------------
        def main():
            ap = argparse.ArgumentParser(description="Evaluate a trained Gemma3 model.")
            ap.add_argument("--model-py", type=str, required=True)
            ap.add_argument("--model-config", required=True)
            ap.add_argument("--model-weights", required=True)
            ap.add_argument("--tokenizer-json", required=True)
            ap.add_argument("--eval-data", required=True)
            ap.add_argument("--experiment_name", default="gemma3_eval")
            ap.add_argument("--max-new-tokens", type=int, default=64)
            ap.add_argument("--temperature", type=float, default=1.0)
            ap.add_argument("--top-k", type=int, default=50)
            ap.add_argument("--random-offset", type=int, default=0)
            ap.add_argument("--metrics-json", default="mlpipeline-metrics.json")
            ap.add_argument("--results-csv", default="evaluation_results.csv")
            args = ap.parse_args()

            print("[INFO] Starting Gemma3 model evaluation...")
            print(f"[INFO] Model config: {args.model_config}")
            print(f"[INFO] Model weights: {args.model_weights}")
            print(f"[INFO] Eval data: {args.eval_data}")
            print(f"[INFO] Temperature: {args.temperature}, Top-K: {args.top_k}")

            # Initialize MLflow
            mlflow.set_experiment(args.experiment_name)
            with mlflow.start_run():
                mlflow.log_params({
                    "model_py": args.model_py,
                    "model_config": args.model_config,
                    "model_weights": args.model_weights,
                    "tokenizer_json": args.tokenizer_json,
                    "eval_data": args.eval_data,
                    "max_new_tokens": args.max_new_tokens,
                    "temperature": args.temperature,
                    "top_k": args.top_k,
                    "random_offset": args.random_offset
                })

                # Set device
                device = "cuda" if torch.cuda.is_available() else "cpu"
                torch_device = torch.device(device)
                model_dtype = torch.float16 if device == "cuda" else torch.float32
                print(f"[INFO] Using device: {device}")

                # Load tokenizer
                print("[INFO] Loading tokenizer...")
                tok = Tokenizer.from_file(args.tokenizer_json)
                bos_id = tok.token_to_id("<s>") or tok.token_to_id("<bos>")
                eos_id = tok.token_to_id("</s>") or tok.token_to_id("<eos>")
                print(f"[INFO] Tokenizer vocab size: {tok.get_vocab_size()}")

                # Load model config
                with open(args.model_config, "r", encoding="utf-8") as f:
                    cfg = json.load(f)
                cfg["dtype"] = model_dtype
                context_len = int(cfg.get("context_length", 2048))
                print(f"[INFO] Model config: {cfg}")

                # Load model
                print("[INFO] Loading model...")
                model_py_path = find_model_py_file(args.model_py)
                print(f"[INFO] Loading model code from: {model_py_path}")
                mod = load_module_from_path(model_py_path, "gemma3_model")
                Gemma3Model = getattr(mod, "Gemma3Model")

                model = Gemma3Model(cfg).to(torch_device)
                state = torch.load(args.model_weights, map_location=torch_device)
                model.load_state_dict(state, strict=True)
                model.eval()
                print("[INFO] Model loaded successfully")

                # Prepare evaluation data
                print("[INFO] Preparing evaluation data...")
                test_data = prepare_eval_data(args.eval_data, tok, args.random_offset)
                print(f"[INFO] Processing {len(test_data)} test cases")

                # Initialize scorers
                rouge_sc = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
                bleu_metric = BLEU()

                # Generate predictions and calculate scores
                results = []
                rouge_scores_list = []
                bleu_scores_list = []
                
                for idx, (input_text, expected_output) in enumerate(test_data):
                    print(f"[INFO] Processing sample {idx+1}/{len(test_data)}")
                    
                    # Generate output
                    generated_output = generate_text(
                        model, tok, input_text, bos_id, eos_id, context_len,
                        args.max_new_tokens, args.temperature, args.top_k
                    )
                    
                    # Calculate ROUGE
                    rouge_scores = rouge_sc.score(expected_output, generated_output)
                    rouge_score = rouge_scores['rougeL'].fmeasure
                    rouge_scores_list.append(rouge_score)
                    
                    # Calculate BLEU
                    bleu_score = bleu_metric.sentence_score(generated_output, [expected_output]).score / 100.0
                    bleu_scores_list.append(bleu_score)
                    
                    result = {
                        "prompt": input_text,
                        "ground_truth": expected_output,
                        "prediction": generated_output,
                        "rouge_score": round(rouge_score, 4),
                        "bleu_score": round(bleu_score, 4)
                    }
                    results.append(result)
                    
                    print(f"  Generated: {generated_output[:100]}...")
                    print(f"  ROUGE: {rouge_score:.4f}, BLEU: {bleu_score:.4f}")

                # Calculate average scores
                avg_rouge = sum(rouge_scores_list) / len(rouge_scores_list) if rouge_scores_list else 0.0
                avg_bleu = sum(bleu_scores_list) / len(bleu_scores_list) if bleu_scores_list else 0.0

                print(f"[INFO] Average ROUGE-L: {avg_rouge:.4f}")
                print(f"[INFO] Average BLEU: {avg_bleu:.4f}")

                # Prepare metrics
                scalar_metrics = {
                    "bleu_score": float(avg_bleu),
                    "rouge_score": float(avg_rouge),
                }

                # Log to MLflow
                mlflow.log_metrics(scalar_metrics)
                print("[INFO] Metrics logged to MLflow")

                # Save results CSV
                results_df = pd.DataFrame(results)
                os.makedirs(os.path.dirname(args.results_csv) or ".", exist_ok=True)
                results_df.to_csv(args.results_csv, index=False)
                mlflow.log_artifact(args.results_csv)
                print(f"[INFO] Results saved to {args.results_csv}")

                # Write KFP Metrics schema (UI-compatible)
                kfp_metrics = {
                    "bleu_score": scalar_metrics["bleu_score"],
                    "rouge_score": scalar_metrics["rouge_score"],
                }
                os.makedirs(os.path.dirname(args.metrics_json) or ".", exist_ok=True)
                with open(args.metrics_json, "w", encoding="utf-8") as f:
                    json.dump(kfp_metrics, f, indent=2)
                print(f"[INFO] KFP metrics saved to {args.metrics_json}")

        if __name__ == "__main__":
            main()
    args:
      - --model-py
      - {inputPath: model_py}
      - --model-config
      - {inputPath: model_config}
      - --model-weights
      - {inputPath: model_weights}
      - --tokenizer-json
      - {inputPath: tokenizer_json}
      - --eval-data
      - {inputPath: eval_data}
      - --experiment_name
      - {inputValue: experiment_name}
      - --max-new-tokens
      - {inputValue: max_new_tokens}
      - --temperature
      - {inputValue: temperature}
      - --top-k
      - {inputValue: top_k}
      - --random-offset
      - {inputValue: random_offset}
      - --metrics-json
      - {outputPath: metrics_json}
      - --results-csv
      - {outputPath: results_csv}
