name: Evaluate Gemma3 Model
description: Evaluates a trained Gemma3 model using perplexity, BLEU, and ROUGE metrics, logs results to MLflow, and supports JSONL or plain-text eval files.
inputs:
  - name: model_py
    type: Data
    description: "Python file OR directory containing Gemma3 model class"
  - name: model_config
    type: Data
    description: "JSON config file for the model"
  - name: model_weights
    type: Model
    description: "Trained weights (.pth file)"
  - name: tokenizer_json
    type: Model
    description: "Tokenizer JSON file with [PAD],[UNK],[BOS],[EOS] tokens"
  - name: eval_data
    type: Data
    description: "Evaluation data: JSONL with 'prompt' & 'ground_truth' OR plain text (one prompt per line)"
  - name: experiment_name
    type: String
    default: "gemma3_eval"
    description: "MLflow experiment name (optional)"
outputs:
  - name: metrics_json
    type: Metrics
    description: "KFP Metrics JSON (UI-compatible)"
  - name: results_csv
    type: Data
    description: "CSV with prompts, ground truth, and predictions"
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - bash
      - -ec
      - |-
        set -o pipefail

        python3 -u - <<'PY'
        import subprocess, sys
        pkgs = ["mlflow", "rouge-score", "nltk", "pandas"]
        for p in pkgs:
            try:
                __import__(p.replace("-", "_"))
                print(f"[INFO] Python package already present: {p}")
            except Exception:
                print(f"[INFO] Installing Python package: {p}")
                subprocess.check_call([sys.executable, "-m", "pip", "install", "--no-cache-dir", p])
        
        # Download NLTK data
        import nltk
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('punkt_tab', quiet=True)
            print("[INFO] NLTK data downloaded successfully")
        except Exception as e:
            print(f"[WARNING] Could not download NLTK data: {e}")
        
        import argparse, json, os, math, importlib.util
        import torch
        import pandas as pd
        import mlflow
        from tokenizers import Tokenizer
        from nltk.translate.bleu_score import corpus_bleu
        from rouge_score import rouge_scorer

        # ----------------- helpers -----------------
        def find_model_py_file(path_or_dir):
            if os.path.isfile(path_or_dir):
                return path_or_dir
            if not os.path.isdir(path_or_dir):
                raise ValueError(f"{path_or_dir} is not a file or dir")
            py_files = [os.path.join(path_or_dir, f) for f in os.listdir(path_or_dir) if f.endswith(".py")]
            if not py_files:
                raise FileNotFoundError(f"No .py file found in {path_or_dir}")
            return py_files[0]

        def load_model_class_from_file(py_path, class_name="Gemma3Model"):
            spec = importlib.util.spec_from_file_location("gemma3_model", py_path)
            if spec is None or spec.loader is None:
                raise ImportError(f"Could not load spec from {py_path}")
            mod = importlib.util.module_from_spec(spec)
            sys.modules["gemma3_model"] = mod
            spec.loader.exec_module(mod)
            if not hasattr(mod, class_name):
                raise AttributeError(f"{py_path} does not define {class_name}")
            return getattr(mod, class_name)

        def generate_text(model, tokenizer, prompt, max_length=50):
            model.eval()
            device = next(model.parameters()).device
            
            # Encode prompt
            input_ids = tokenizer.encode(prompt, add_special_tokens=True).ids
            generated_ids = list(input_ids)
            
            # Get special token IDs
            eos_id = tokenizer.token_to_id("[EOS]")
            pad_id = tokenizer.token_to_id("[PAD]")
            
            with torch.no_grad():
                for _ in range(max_length - len(input_ids)):
                    current_input = torch.tensor([generated_ids], device=device)
                    
                    # Get model output - handle both tuple and tensor returns
                    output = model(current_input)
                    if isinstance(output, tuple):
                        logits = output[0]  # (batch, seq_len, vocab_size)
                    else:
                        logits = output
                    
                    # Get next token
                    next_token_id = torch.argmax(logits[:, -1, :], dim=-1).item()
                    
                    # Stop at EOS or PAD
                    if (eos_id is not None and next_token_id == eos_id) or \
                       (pad_id is not None and next_token_id == pad_id):
                        break
                    
                    generated_ids.append(next_token_id)
            
            # Decode and return
            return tokenizer.decode(generated_ids, skip_special_tokens=True)

        def calculate_perplexity(model, tokenizer, data, device):
            model.eval()
            total_loss, total_tokens = 0.0, 0
            loss_fn = torch.nn.CrossEntropyLoss(reduction='sum', ignore_index=tokenizer.token_to_id("[PAD]") or -100)
            
            for idx, row in data.iterrows():
                # Combine prompt and ground truth for perplexity calculation
                text = row["prompt"]
                if row.get("ground_truth") and str(row["ground_truth"]).strip():
                    text += " " + str(row["ground_truth"])
                
                # Encode text
                ids = tokenizer.encode(text, add_special_tokens=True).ids
                if len(ids) < 2:
                    continue
                
                input_tensor = torch.tensor([ids], device=device)
                
                with torch.no_grad():
                    try:
                        # Try to get loss from model if it supports labels
                        output = model(input_tensor, labels=input_tensor)
                        if isinstance(output, tuple) and len(output) > 1 and output[1] is not None:
                            loss = output[1]
                            tokens = max(0, input_tensor.size(1) - 1)
                        else:
                            # Calculate loss manually
                            logits = output[0] if isinstance(output, tuple) else output
                            # Shift for next-token prediction
                            shift_logits = logits[:, :-1, :].contiguous()
                            shift_labels = input_tensor[:, 1:].contiguous()
                            loss = loss_fn(
                                shift_logits.view(-1, shift_logits.size(-1)),
                                shift_labels.view(-1)
                            )
                            tokens = shift_labels.numel()
                    except Exception as e:
                        print(f"[WARNING] Error calculating loss for sample {idx}: {e}")
                        continue
                
                if loss is not None and tokens > 0:
                    total_loss += loss.item()
                    total_tokens += tokens
            
            if total_tokens == 0:
                print("[WARNING] No valid tokens for perplexity calculation")
                return float("inf")
            
            avg_loss = total_loss / total_tokens
            perplexity = math.exp(avg_loss) if avg_loss < 100 else float("inf")
            return perplexity

        def calculate_bleu(model, tokenizer, data):
            predictions, references = [], []
            pred_texts = []
            
            for idx, row in data.iterrows():
                try:
                    pred = generate_text(model, tokenizer, row["prompt"], max_length=100)
                    pred_texts.append(pred)
                    predictions.append(pred.split())
                    
                    # Handle ground truth
                    gt = str(row.get("ground_truth") or "").strip()
                    references.append([gt.split()])
                except Exception as e:
                    print(f"[WARNING] Error generating text for sample {idx}: {e}")
                    pred_texts.append("")
                    predictions.append([])
                    references.append([[]])
            
            # Calculate BLEU score
            try:
                bleu = corpus_bleu(references, predictions)
            except Exception as e:
                print(f"[WARNING] Error calculating BLEU: {e}")
                bleu = 0.0
            
            return bleu, pred_texts

        def calculate_rouge(model, tokenizer, data):
            scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
            r1s, r2s, rLs = [], [], []
            
            for idx, row in data.iterrows():
                try:
                    pred = generate_text(model, tokenizer, row["prompt"], max_length=100)
                    gt = str(row.get("ground_truth") or "").strip()
                    
                    # Skip if both are empty
                    if not pred and not gt:
                        continue
                    
                    scores = scorer.score(gt if gt else " ", pred if pred else " ")
                    r1s.append(scores["rouge1"].fmeasure)
                    r2s.append(scores["rouge2"].fmeasure)
                    rLs.append(scores["rougeL"].fmeasure)
                except Exception as e:
                    print(f"[WARNING] Error calculating ROUGE for sample {idx}: {e}")
                    continue
            
            def avg(xs): return sum(xs) / len(xs) if xs else 0.0
            return avg(r1s), avg(r2s), avg(rLs)

        def load_eval_data(path):
            try:
                # Try loading as JSONL
                df = pd.read_json(path, lines=True)
                if {"prompt", "ground_truth"}.issubset(df.columns):
                    print(f"[INFO] Loaded {len(df)} samples from JSONL with prompt and ground_truth")
                    return df
                if "prompt" in df.columns and "ground_truth" not in df.columns:
                    df = df.copy()
                    df["ground_truth"] = ""
                    print(f"[INFO] Loaded {len(df)} samples from JSONL with prompt only")
                    return df
            except Exception as e:
                print(f"[INFO] Could not load as JSONL: {e}")
            
            # Load as plain text
            try:
                with open(path, "r", encoding="utf-8") as f:
                    lines = [l.strip() for l in f if l.strip()]
                print(f"[INFO] Loaded {len(lines)} prompts from plain text file")
                return pd.DataFrame({"prompt": lines, "ground_truth": [""] * len(lines)})
            except Exception as e:
                raise ValueError(f"Could not load eval data from {path}: {e}")

        # ----------------- main -----------------
        def main():
            ap = argparse.ArgumentParser(description="Evaluate a trained Gemma3 model.")
            ap.add_argument("--model-py", required=True)
            ap.add_argument("--model-config", required=True)
            ap.add_argument("--model-weights", required=True)
            ap.add_argument("--tokenizer-json", required=True)
            ap.add_argument("--eval-data", required=True)
            ap.add_argument("--experiment_name", default="gemma3_eval")
            ap.add_argument("--metrics-json", default="mlpipeline-metrics.json")
            ap.add_argument("--results-csv", default="evaluation_results.csv")
            args = ap.parse_args()

            print("[INFO] Starting Gemma3 model evaluation...")
            print(f"[INFO] Model config: {args.model_config}")
            print(f"[INFO] Model weights: {args.model_weights}")
            print(f"[INFO] Eval data: {args.eval_data}")

            # Initialize MLflow
            mlflow.set_experiment(args.experiment_name)
            with mlflow.start_run():
                mlflow.log_params({
                    "model_py": args.model_py,
                    "model_config": args.model_config,
                    "model_weights": args.model_weights,
                    "tokenizer_json": args.tokenizer_json,
                    "eval_data": args.eval_data
                })

                # Set device
                device = "cuda" if torch.cuda.is_available() else "cpu"
                print(f"[INFO] Using device: {device}")

                # Load model config
                with open(args.model_config, "r") as f:
                    config = json.load(f)
                
                # Handle dtype
                if config.get("dtype") == "float16" and device == "cuda":
                    config["dtype"] = torch.float16
                else:
                    config["dtype"] = torch.float32
                
                print(f"[INFO] Model config: {config}")

                # Load model
                print("[INFO] Loading model...")
                model_file = find_model_py_file(args.model_py)
                print(f"[INFO] Model file: {model_file}")
                ModelClass = load_model_class_from_file(model_file, "Gemma3Model")
                model = ModelClass(config).to(device)
                
                # Load weights
                state_dict = torch.load(args.model_weights, map_location=device)
                model.load_state_dict(state_dict)
                model.eval()
                print("[INFO] Model loaded successfully")

                # Load tokenizer
                print("[INFO] Loading tokenizer...")
                tokenizer = Tokenizer.from_file(args.tokenizer_json)
                print(f"[INFO] Tokenizer vocab size: {tokenizer.get_vocab_size()}")
                print(f"[INFO] Special tokens: [PAD]={tokenizer.token_to_id('[PAD]')}, [UNK]={tokenizer.token_to_id('[UNK]')}, [BOS]={tokenizer.token_to_id('[BOS]')}, [EOS]={tokenizer.token_to_id('[EOS]')}")

                # Load evaluation data
                print("[INFO] Loading evaluation data...")
                eval_df = load_eval_data(args.eval_data)
                print(f"[INFO] Evaluation dataset size: {len(eval_df)}")

                # Calculate metrics
                print("[INFO] Calculating perplexity...")
                perplexity = calculate_perplexity(model, tokenizer, eval_df, device)
                print(f"[INFO] Perplexity: {perplexity:.4f}")

                print("[INFO] Calculating BLEU score...")
                bleu, preds = calculate_bleu(model, tokenizer, eval_df)
                print(f"[INFO] BLEU Score: {bleu:.4f}")

                print("[INFO] Calculating ROUGE scores...")
                r1, r2, rL = calculate_rouge(model, tokenizer, eval_df)
                print(f"[INFO] ROUGE-1: {r1:.4f}, ROUGE-2: {r2:.4f}, ROUGE-L: {rL:.4f}")

                # Prepare metrics
                scalar_metrics = {
                    "perplexity": float(perplexity),
                    "bleu_score": float(bleu),
                    "rouge1_f1": float(r1),
                    "rouge2_f1": float(r2),
                    "rougeL_f1": float(rL),
                }

                # Log to MLflow
                mlflow.log_metrics(scalar_metrics)
                print("[INFO] Metrics logged to MLflow")

                # Save predictions table
                results_df = pd.DataFrame({
                    "prompt": eval_df["prompt"],
                    "ground_truth": eval_df["ground_truth"],
                    "prediction": preds
                })
                results_df.to_csv(args.results_csv, index=False)
                mlflow.log_artifact(args.results_csv)
                print(f"[INFO] Results saved to {args.results_csv}")

                # Write KFP Metrics schema (UI-compatible)
                kfp_metrics = {
                    "metrics": [
                        {"name": "perplexity", "numberValue": scalar_metrics["perplexity"], "format": "RAW"},
                        {"name": "bleu_score", "numberValue": scalar_metrics["bleu_score"], "format": "PERCENTAGE"},
                        {"name": "rouge1_f1", "numberValue": scalar_metrics["rouge1_f1"], "format": "PERCENTAGE"},
                        {"name": "rouge2_f1", "numberValue": scalar_metrics["rouge2_f1"], "format": "PERCENTAGE"},
                        {"name": "rougeL_f1", "numberValue": scalar_metrics["rougeL_f1"], "format": "PERCENTAGE"},
                    ]
                }
                os.makedirs(os.path.dirname(args.metrics_json) or ".", exist_ok=True)
                with open(args.metrics_json, "w", encoding="utf-8") as f:
                    json.dump(kfp_metrics, f, indent=2)
                print(f"[INFO] KFP metrics saved to {args.metrics_json}")



        if __name__ == "__main__":
            main()
        PY
    args:
      - --model-py
      - {inputPath: model_py}
      - --model-config
      - {inputPath: model_config}
      - --model-weights
      - {inputPath: model_weights}
      - --tokenizer-json
      - {inputPath: tokenizer_json}
      - --eval-data
      - {inputPath: eval_data}
      - --experiment_name
      - {inputValue: experiment_name}
      - --metrics-json
      - {outputPath: metrics_json}
      - --results-csv
      - {outputPath: results_csv}
