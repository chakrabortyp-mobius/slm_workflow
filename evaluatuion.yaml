name: Evaluate Gemma3 Model
description: Evaluates a trained Gemma3 model using perplexity, BLEU, and ROUGE metrics, logs results to MLflow, and supports JSONL or plain-text eval files.
inputs:
  - name: model_py
    type: Data
    description: "Python file OR directory containing Gemma3 model class"
  - name: model_config
    type: Data
    description: "JSON config file for the model"
  - name: model_weights
    type: Model
    description: "Trained weights (.pth file)"
  - name: tokenizer_json
    type: Model
    description: "Tokenizer JSON file"
  - name: eval_data
    type: Data
    description: "Evaluation data: JSONL with 'prompt' & 'ground_truth' OR plain text (one prompt per line)"
  - name: experiment_name
    type: String
    default: "gemma3_eval"
    description: "MLflow experiment name (optional)"
outputs:
  - name: metrics_json
    type: Metrics
    description: "KFP Metrics JSON (UI-compatible)"
  - name: results_csv
    type: Data
    description: "CSV with prompts, ground truth, and predictions"
implementation:
  container:
    image: nikhilv215/nesy-factory:v22
    command:
      - bash
      - -ec
      - |-
        set -o pipefail

        python3 -u - <<'PY'
        import argparse, json, os, math, importlib.util
        import torch
        import pandas as pd
        import mlflow
        from tokenizers import Tokenizer
        from nltk.translate.bleu_score import corpus_bleu
        from rouge_score import rouge_scorer

        # ----------------- helpers -----------------
        def find_model_py_file(path_or_dir):
            if os.path.isfile(path_or_dir):
                return path_or_dir
            if not os.path.isdir(path_or_dir):
                raise ValueError(f"{path_or_dir} is not a file or dir")
            py_files = [os.path.join(path_or_dir, f) for f in os.listdir(path_or_dir) if f.endswith(".py")]
            if not py_files:
                raise FileNotFoundError(f"No .py file found in {path_or_dir}")
            return py_files[0]

        def load_model_class_from_file(py_path, class_name="Gemma3Model"):
            spec = importlib.util.spec_from_file_location("gemma3_model", py_path)
            if spec is None or spec.loader is None:
                raise ImportError(f"Could not load spec from {py_path}")
            mod = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(mod)
            if not hasattr(mod, class_name):
                raise AttributeError(f"{py_path} does not define {class_name}")
            return getattr(mod, class_name)

        def generate_text(model, tokenizer, prompt, max_length=50):
            model.eval()
            device = next(model.parameters()).device
            input_ids = tokenizer.encode(prompt, add_special_tokens=True).ids
            generated_ids = list(input_ids)
            with torch.no_grad():
                for _ in range(max_length - len(input_ids)):
                    current_input = torch.tensor([generated_ids], device=device)
                    logits, _ = model(current_input)
                    next_token_id = torch.argmax(logits[:, -1, :], dim=-1).item()
                    eos_id = tokenizer.token_to_id("[EOS]")
                    if eos_id is not None and next_token_id == eos_id:
                        break
                    generated_ids.append(next_token_id)
            return tokenizer.decode(generated_ids)

        def calculate_perplexity(model, tokenizer, data, device):
            total_loss, total_tokens = 0.0, 0
            for _, row in data.iterrows():
                text = row["prompt"] + ((" " + row["ground_truth"]) if row.get("ground_truth") else "")
                ids = tokenizer.encode(text, add_special_tokens=True).ids
                input_tensor = torch.tensor([ids], device=device)
                labels = input_tensor.clone()
                with torch.no_grad():
                    _, loss = model(input_tensor, labels=labels)
                if loss is not None:
                    tokens = max(0, input_tensor.size(1) - 1)
                    total_loss += loss.item() * tokens
                    total_tokens += tokens
            avg_loss = (total_loss / total_tokens) if total_tokens > 0 else 0.0
            return math.exp(avg_loss) if avg_loss > 0 else float("inf")

        def calculate_bleu(model, tokenizer, data):
            predictions, references = [], []
            for _, row in data.iterrows():
                pred = generate_text(model, tokenizer, row["prompt"])
                predictions.append(pred.split())
                gt = (row.get("ground_truth") or "").split()
                references.append([gt])
            bleu = corpus_bleu(references, predictions)
            return bleu, [" ".join(p) for p in predictions]

        def calculate_rouge(model, tokenizer, data):
            scorer = rouge_scorer.RougeScorer(["rouge1", "rouge2", "rougeL"], use_stemmer=True)
            r1s, r2s, rLs = [], [], []
            for _, row in data.iterrows():
                pred = generate_text(model, tokenizer, row["prompt"])
                gt = row.get("ground_truth") or ""
                scores = scorer.score(gt, pred)
                r1s.append(scores["rouge1"].fmeasure)
                r2s.append(scores["rouge2"].fmeasure)
                rLs.append(scores["rougeL"].fmeasure)
            def avg(xs): return sum(xs) / len(xs) if xs else 0.0
            return avg(r1s), avg(r2s), avg(rLs)

        def load_eval_data(path):
            try:
                df = pd.read_json(path, lines=True)
                if {"prompt", "ground_truth"}.issubset(df.columns):
                    return df
                if "prompt" in df.columns and "ground_truth" not in df.columns:
                    df = df.copy()
                    df["ground_truth"] = ""
                    return df
            except Exception:
                pass
            with open(path, "r", encoding="utf-8") as f:
                lines = [l.strip() for l in f if l.strip()]
            return pd.DataFrame({"prompt": lines, "ground_truth": [""] * len(lines)})

        # ----------------- main -----------------
        def main():
            ap = argparse.ArgumentParser(description="Evaluate a trained Gemma3 model.")
            ap.add_argument("--model-py", required=True)
            ap.add_argument("--model-config", required=True)
            ap.add_argument("--model-weights", required=True)
            ap.add_argument("--tokenizer-json", required=True)
            ap.add_argument("--eval-data", required=True)
            ap.add_argument("--experiment_name", default="gemma3_eval")
            ap.add_argument("--metrics-json", default="mlpipeline-metrics.json")
            ap.add_argument("--results-csv", default="evaluation_results.csv")
            args = ap.parse_args()

            mlflow.set_experiment(args.experiment_name)
            with mlflow.start_run():
                mlflow.log_params({
                    "model_py": args.model_py,
                    "model_config": args.model_config,
                    "model_weights": args.model_weights,
                    "tokenizer_json": args.tokenizer_json,
                    "eval_data": args.eval_data
                })

                device = "cuda" if torch.cuda.is_available() else "cpu"

                with open(args.model_config, "r") as f:
                    config = json.load(f)
                config["dtype"] = torch.float16 if (config.get("dtype") == "float16" and device == "cuda") else torch.float32

                model_file = find_model_py_file(args.model_py)
                ModelClass = load_model_class_from_file(model_file, "Gemma3Model")
                model = ModelClass(config).to(device)
                model.load_state_dict(torch.load(args.model_weights, map_location=device))
                model.eval()

                tokenizer = Tokenizer.from_file(args.tokenizer_json)
                if tokenizer.token_to_id("[EOS]") is None:
                    tokenizer.add_special_tokens(["[EOS]"])

                eval_df = load_eval_data(args.eval_data)

                perplexity = calculate_perplexity(model, tokenizer, eval_df, device)
                bleu, preds = calculate_bleu(model, tokenizer, eval_df)
                r1, r2, rL = calculate_rouge(model, tokenizer, eval_df)

                # Log to MLflow
                scalar_metrics = {
                    "perplexity": float(perplexity),
                    "bleu_score": float(bleu),
                    "rouge1_f1": float(r1),
                    "rouge2_f1": float(r2),
                    "rougeL_f1": float(rL),
                }
                mlflow.log_metrics(scalar_metrics)

                # Save predictions table
                results_df = pd.DataFrame({
                    "prompt": eval_df["prompt"],
                    "ground_truth": eval_df["ground_truth"],
                    "prediction": preds
                })
                results_df.to_csv(args.results_csv, index=False)
                mlflow.log_artifact(args.results_csv)

                # Write KFP Metrics schema (UI-compatible)
                kfp_metrics = {
                    "metrics": [
                        {"name": "perplexity", "numberValue": scalar_metrics["perplexity"], "format": "RAW"},
                        {"name": "bleu_score", "numberValue": scalar_metrics["bleu_score"], "format": "PERCENTAGE"},
                        {"name": "rouge1_f1", "numberValue": scalar_metrics["rouge1_f1"], "format": "PERCENTAGE"},
                        {"name": "rouge2_f1", "numberValue": scalar_metrics["rouge2_f1"], "format": "PERCENTAGE"},
                        {"name": "rougeL_f1", "numberValue": scalar_metrics["rougeL_f1"], "format": "PERCENTAGE"},
                    ]
                }
                os.makedirs(os.path.dirname(args.metrics_json) or ".", exist_ok=True)
                with open(args.metrics_json, "w", encoding="utf-8") as f:
                    json.dump(kfp_metrics, f, indent=2)

                print(json.dumps(scalar_metrics, indent=4))

        if __name__ == "__main__":
            main()
        PY
    args:
      - --model-py
      - {inputPath: model_py}
      - --model-config
      - {inputPath: model_config}
      - --model-weights
      - {inputPath: model_weights}
      - --tokenizer-json
      - {inputPath: tokenizer_json}
      - --eval-data
      - {inputPath: eval_data}
      - --experiment_name
      - {inputValue: experiment_name}
      - --metrics-json
      - {outputPath: metrics_json}
      - --results-csv
      - {outputPath: results_csv}
